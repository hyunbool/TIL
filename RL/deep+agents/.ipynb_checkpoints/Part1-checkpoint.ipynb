{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow: Part 1 - Two-armed Bandit\n",
    "\n",
    "## Two-Armed Bandit\n",
    "\n",
    "* n-armed bandit problem\n",
    "  * n개의 slot machine이 존재\n",
    "    * 각각의 머신은 다른 payout probability를 가진다.\n",
    "  * 목표: 가장 payout이 큰 기계를 찾아 returned reward를 최대화하는 것.\n",
    "  * 간단하게 설계하기 위해 2개의 머신만을 고려하려 한다.\n",
    "  * Reward:\n",
    "    * 각각의 action은 다른 reward를 가진다.\n",
    "      * eg) 미로 안에서 보물을 찾는 상황에서 왼쪽으로 가면 보물을 찾게되지만, 오른쪽으로 가면 뱀들을 만나게 되는 상g황\n",
    "    * reward는 delay 되어 계산된다.\n",
    "      * eg) 현재 상황에서 왼쪽으로 가면 보물을 찾게 되어도, 바로 잘 찾았는지 알지 못한다. 나중에 알게 된다.\n",
    "    * Reward for an action is conditional on the state of the environment.\n",
    "  * 하지만 n-armed bandit에서는 두번째와 세번째 조건을 고려할 필요가 없다.\n",
    "    * 가능한 action에 대해 어떤 reward를 받을수 있는지를 학습하고, 가장 좋은 reward를 얻을 수 있도록 하는데만 초점을 맞추면 된다. => **policy learning**\n",
    "  * policy gradient를 사용해 environment로부터 얻은 피드백을 이용해 gradient descent를 통해 weight를 조정하여 Neural Network가 action을 선택할 수 있는 policy를 학습한다.\n",
    "    * 이렇게 주어진 state에 대한 optimal action을 찾는 것을 학습하는 것이 아니라, **value function을 이용해  agent가 주어진 state나 action이 얼마나 좋은지를 예측하도록 학습하는 방법도 있다.**\n",
    "      * 하지만 policy gradient가 조금 더 직접적이다.\n",
    "\n",
    "\n",
    "\n",
    "## Policy Gradient\n",
    "\n",
    "* Policy Gradient 네트워크를 이해하는 가장 간단한 방법은 explicit output을 만들어내는 네트워크를 생각해보는 것이다.\n",
    "  * bandit problem에서는 state에 대한 조건부 output을 계산하지 않아도 된다.\n",
    "    * 네트워크는 weight를 가지고 둘 중 어떤 arm을 당기는 것이 좋은지를 표현하게 된다.\n",
    "* 네트워크 업데이트를 위해 ε-greedy policy를 사용해 arm을 당겨보도록 하자.\n",
    "  * 대부분의 경우 greedy하게 action을 선택하지만 때로는(ε 확률만큼) 랜덤하게 action을 선택한다.\n",
    "    * Agent가 exploration을 더 잘 하게 만들어 준다.\n",
    "* Agent가 action을 취하면 1 혹은 -1의 reward를 받게 된다.\n",
    "  * 이 reward를 가지고 policy loss를 이용해 네트워크를 업데이트 한다.\n",
    "    * Loss = -log(π) * A\n",
    "      * A: advantage\n",
    "      * π: policy, 선택된 action의 weight에 대응된다.\n",
    "  * Loss function을 통해 positive reward를 갖는 action의 weight를 증가시키고, 반대의 경우에는 action의 weight를 감소시킨다.\n",
    "* 이렇게 action을 취하고, reward를 얻고, 네트워크를 업데이트 함으로써 agent가 수렴하도록 만들어준다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Reinforement Learning in Tensorflow Part 1: The Multi-armed bandit\n",
    "* multi-armed bandit problem을 해결하기 위한 policy-gradient 네트워크 만들기\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Bandits\n",
    "\"\"\"\n",
    "# bandit 정의: four-armed bandit\n",
    "# pullBandit(): 평균이 0인 정규분포에서 난수를 만든다. \n",
    "# - 숫자가 작을수록 positive reward를 만들어 낼 확률이 높다.\n",
    "# - agent가 항상 positive reward를 만들어내는 bandit을 선택하는 것을 목표로 한다.\n",
    "\n",
    "# bandit 리스트\n",
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullBandit(bandit):\n",
    "    # 난수 생성\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        # positive reward\n",
    "        return 1\n",
    "    else:\n",
    "        # negative reward\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Agent\n",
    "- 각 bandit에 대한 값을 가지는 간단한 NN agent\n",
    "- 각 값은 해당 bandit을 선택했을때의 return에 대한 추정값이다.\n",
    "- policy gradient를 사용해 agent를 업데이트\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# network의 feed-forward 부분\n",
    "# 여기서 선택이 일어난다.\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# training 과정에 대한 정의 부분\n",
    "# loss를 계산해 그것을 네트워크 업데이트에 사용하기 위해 reward와 선택된 action을 feed\n",
    "reward_holder = tf.placeholder(shape=[1], dtype = tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "responsible_weight =tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the  4  bandits:  [1. 0. 0. 0.]\n",
      "Running reward for the  4  bandits:  [ 0. -2.  2. 45.]\n",
      "Running reward for the  4  bandits:  [-1. -2.  0. 92.]\n",
      "Running reward for the  4  bandits:  [ -1.  -1.   0. 141.]\n",
      "Running reward for the  4  bandits:  [ -1.  -1.   3. 188.]\n",
      "Running reward for the  4  bandits:  [ -1.  -2.   3. 237.]\n",
      "Running reward for the  4  bandits:  [  0.  -2.   4. 281.]\n",
      "Running reward for the  4  bandits:  [  0.  -2.   4. 327.]\n",
      "Running reward for the  4  bandits:  [ -1.  -2.   6. 372.]\n",
      "Running reward for the  4  bandits:  [ -2.  -4.   7. 416.]\n",
      "Running reward for the  4  bandits:  [ -2.  -4.   7. 466.]\n",
      "Running reward for the  4  bandits:  [ -2.  -5.   6. 512.]\n",
      "Running reward for the  4  bandits:  [ -2.  -5.   6. 554.]\n",
      "Running reward for the  4  bandits:  [ -2.  -4.   4. 599.]\n",
      "Running reward for the  4  bandits:  [ -1.  -2.   5. 645.]\n",
      "Running reward for the  4  bandits:  [ -3.  -1.   5. 692.]\n",
      "Running reward for the  4  bandits:  [ -4.  -2.   5. 736.]\n",
      "Running reward for the  4  bandits:  [ -5.   0.   4. 782.]\n",
      "Running reward for the  4  bandits:  [ -5.  -1.   4. 831.]\n",
      "Running reward for the  4  bandits:  [ -6.  -3.   3. 877.]\n",
      "The agent thinks bandit: 3 is the most promissing...\n",
      "... and it was right!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Agent 학습\n",
    "\"\"\"\n",
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while(i < total_episodes):\n",
    "        if np.random.rand(1) < e:\n",
    "            action =  np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        # 선택한 bandit에 대한 reward 얻기\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        # update networks\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights], feed_dict = {reward_holder: [reward], action_holder: [action]})\n",
    "        \n",
    "        # update scores\n",
    "        total_reward[action] += reward\n",
    "        \n",
    "        if(i % 50 == 0):\n",
    "            print(\"Running reward for the \", str(num_bandits), \" bandits: \", str(total_reward))\n",
    "        i += 1\n",
    "    \n",
    "print(\"The agent thinks bandit: \" + str(np.argmax(ww)) + \" is the most promissing...\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"... and it was right!\")\n",
    "else: print(\"... and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
