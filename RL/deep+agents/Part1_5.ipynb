{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow Part 1.5: Contextual Bandits\n",
    "\n",
    "* 앞서 multi-armed bandit problem를 해결하기 위해 구현해보았던 agent는 environmental state를 고려하지 않고 단순히 어떤 action을 고를지를 학습하는 형태\n",
    "    * state가 주어지지 않으면, 어떤 순간에서의 best action이 항상 best action이 된다.\n",
    "    * 따라서 part2에서는 environmental state, 이전 action에 기반한 새로운 state, delayed reward를 고려한 full RL 문제를 해결할 것이다.\n",
    "* 일단 그 전에 state는 존재 하지만, 이전 state나 action에 의해 결정되지 않으며 delayed reward도 고려하지 않는 경우를 살펴보자. ==> **Contextual Bandit**\n",
    "    * Multi-armed bandit problem, where only action effect reward.\n",
    "    * Middle: Contextual bandit problem, where state and action effect reward.\n",
    "    * Bottom: Full RL problem, where action effects state, and rewards may be delayed in time.\n",
    "<img src=\"https://miro.medium.com/max/700/1*3NziBtrANN6UVltplxwaGA.png\"/>\n",
    "\n",
    "## Contextual Bandit\n",
    "* multi-armed bandit: action을 취하면, 그 action에 따른 reward가 각각 다른 비율로 계산된다.\n",
    "    * 하지만 만약 agent가 positive reward를 줄 가능성이 높은 arm을 항상 선택한다면?\n",
    "        * environment의 state가 존재하지 않게 된다. 즉 바뀌지 않는 하나의 state로 고정되게된다.\n",
    "* Contextual Bandit은 여기에 state라는 개념을 추가한 것이다.\n",
    "    * state는 agent가 어떤 action을 취해야 할지 결정할 때 사용할 수 있는 환경에 대한 정보를 포함한다.\n",
    "    * single bandit에서 multiple bandit으로 넓혀서 생각해 보자.\n",
    "        * environment의 state는 우리가 현재 어떤 bandit을 사용하고 있으며, 하나의 bandit에 대해서만 best action을 취할 것이 아니라 다른 모든 bandit에 대해서도 좋은 결과를 낳는 것이 agent의 목표라는 것을 알려준다.\n",
    "        * 각 bandit은 각 arm에 대해 다른 reward probability를 가지기 때문에 우리의 agent는 state에 따른 action 변화가 어떻게 될 지를 학습해야 한다.\n",
    "* single-layer neural network를 이용해 state에 따른 action을 알아보자.\n",
    "    * policy-gradient를 사용해 reward를 최대화 시킬 수 있는 action을 취하도록 만들 수 있을 것이다.\n",
    "\n",
    "## Simple Reinforcement Learning in Tensorflow Part 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Contextual Bandits\n",
    "\"\"\"\n",
    "# contextual bandit problem을 풀 수 있는\n",
    "# policy-gradient 기반의 agent example\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개의 four-armed bandit을 사용\n",
    "# 각 bandit은 각 arm에 대해 다른 success probability를 가지고 있다.\n",
    "# 따라서 best result를 내는데 arm 별로 각기 다른 action을 요구한다.\n",
    "# pullBandit(): 정규분포에서 난수를 만들어 내는 함수\n",
    "# - bandit number가 작을수록 양수의 reward를 만들어 낼 가능성이 높다.\n",
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        # bandit 리스트\n",
    "        # 현재로서는 각각 4, 2, 1번째 arm이 optimal\n",
    "        self.bandits = np.array([[0.2, 0, -0.0, -5], \n",
    "                                 [0.1, -5, 1, 0.25], \n",
    "                                 [-5, 5, 5, 5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "    \n",
    "    def getBandit(self):\n",
    "        # 각 episode에 대해 random state를 리턴\n",
    "        self.state = np.random.randint(0, len(self.bandits))\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self, action):\n",
    "        # 랜덤 숫자 얻기\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        \n",
    "        # positive reward\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        # negative reward\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Policy-Based Agent\n",
    "\"\"\"\n",
    "# current state를 input으로 받고, action을 리턴하는 simple neural agent\n",
    "# 이를 통해 agent는 environment의 state에 기반한 action을 취할 수 있게 된다!\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        # network의 feed-forward 부분\n",
    "        # state로부터 action을 수행한다.\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
    "        output = slim.fully_connected(state_in_OH, a_size,\n",
    "                                     biases_initializer = None,\n",
    "                                     activation_fn = tf.nn.sigmoid,\n",
    "                                     weights_initializer = tf.ones_initializer())\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # training\n",
    "        # reward를 feed해 action을 선택한다\n",
    "        # loss를 계산하기 위해 network를 업데이트하는데 사용한다.\n",
    "        self.reward_holder = tf.placeholder(shape=[1], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13e667b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13e667b10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13e667b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13e667b10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Mean reward for  3  bandits:  [-0.25  0.    0.  ]\n",
      "Mean reward for  3  bandits:  [37.25 41.5  32.  ]\n",
      "Mean reward for  3  bandits:  [72.75 80.5  68.5 ]\n",
      "Mean reward for  3  bandits:  [110.   114.5  106.25]\n",
      "Mean reward for  3  bandits:  [151.5  146.75 144.  ]\n",
      "Mean reward for  3  bandits:  [192.75 185.5  173.5 ]\n",
      "Mean reward for  3  bandits:  [233.   222.25 206.5 ]\n",
      "Mean reward for  3  bandits:  [270.5  262.   242.25]\n",
      "Mean reward for  3  bandits:  [311.   299.75 278.5 ]\n",
      "Mean reward for  3  bandits:  [353.5  331.5  317.25]\n",
      "Mean reward for  3  bandits:  [389.5  372.25 355.  ]\n",
      "Mean reward for  3  bandits:  [429.5  407.   390.75]\n",
      "Mean reward for  3  bandits:  [470.25 445.75 422.25]\n",
      "Mean reward for  3  bandits:  [507.5  486.25 457.5 ]\n",
      "Mean reward for  3  bandits:  [545.   521.   495.75]\n",
      "Mean reward for  3  bandits:  [583.   558.25 529.  ]\n",
      "Mean reward for  3  bandits:  [617.75 602.   562.5 ]\n",
      "Mean reward for  3  bandits:  [654.25 637.75 598.25]\n",
      "Mean reward for  3  bandits:  [691.25 671.25 639.75]\n",
      "Mean reward for  3  bandits:  [732.   711.25 672.  ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "was right\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "was right\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "was right\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training the Agent\n",
    "\"\"\"\n",
    "# environment로부터 state를 얻고, action을 취하고, reward를 얻는 과정으로 agent를 학습한다.\n",
    "# state에 기반한 action을 상황에 따라 선택할 수 있도록 agent를 업데이트 할 수 있게 된다.\n",
    "\n",
    "# tf graph reset\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# bandit 로딩\n",
    "cBandit = contextual_bandit()\n",
    "\n",
    "# Agent 로딩\n",
    "myAgent = agent(lr = 0.001, s_size = cBandit.num_bandits,\n",
    "               a_size = cBandit.num_actions)\n",
    "\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "\n",
    "# bandit score 초기화\n",
    "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
    "e = 0.1 # epsilon\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while(i < total_episodes):\n",
    "        # state를 받아온다.\n",
    "        s = cBandit.getBandit()\n",
    "        \n",
    "        # random action을 취할 것인지 network를 통해 예측할 것인지를 선택\n",
    "        if(np.random.rand(1) < e):\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict = {myAgent.state_in: [s]})\n",
    "\n",
    "        # 현재 bandit에서 취한 action에 따르는 reward를 얻는다.\n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "\n",
    "        # network update\n",
    "        feed_dict = {myAgent.reward_holder: [reward], \\\n",
    "                     myAgent.action_holder: [action], \n",
    "                     myAgent.state_in: [s]}\n",
    "        _, ww = sess.run([myAgent.update, weights], feed_dict = feed_dict)\n",
    "\n",
    "        total_reward[s, action] += reward\n",
    "        if(i % 500 == 0):\n",
    "            print(\"Mean reward for \", str(cBandit.num_bandits), \" bandits: \", str(np.mean(total_reward, axis = 1)))\n",
    "        i+=1\n",
    "for a in range(cBandit.num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "    if(np.argmax(ww[a]) == np.argmin(cBandit.bandits[a])):\n",
    "        print(\"was right\")\n",
    "    else:\n",
    "        print(\"was wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
