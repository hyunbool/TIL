{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents\n",
    "* word에 대한 observation을 한 다음에 현재 뿐만 아니라 장기적 관점에서 optimal reward를 얻을 수 있는 action을 취하는 agent를 만들어보자!\n",
    "    * 이를 통해 full reinforcement agent를 만들어 낼 수 있다.\n",
    "* 이런 environment를 Markov Decision Process(MDP)라고 한다.\n",
    "    * 주어진 action에 대해 reward와 state 변화를 제공할 수 있을 뿐만 아니라 environment의 state와 그 state에서 agent가 취한 action의 reward도 얻을 수 있다.\n",
    "    * MDP는 모든 가능한 state들의 집합 S와 모든 가능한 action들의 집합 A로 구성된다.\n",
    "        * state-action pair (s, a)에 대해\n",
    "            * 새로운 state s'에 대한 transition probability는 T(s, a)로 정의된다.\n",
    "            * reward r는 R(s, a)를 통해 계산된다.\n",
    "        * 즉, MDP에서 state s와 action a가 주어졌을 때 agent는 새로운 state s'와 reward r을 가지게 된다.\n",
    "* eg) opening a door\n",
    "    * 환경에 대한 정보\n",
    "        * state: 우리가 보고 있는 문의 면 & 우리가 바라보고 있는 방향 & 문의 위치\n",
    "        * action: 우리가 취할 수 있는 모든 행동\n",
    "        * reward: 성공적으로 문을 연 경우\n",
    "    * 문으로 걸어가는 행동은 문제를 해결하기 위해 중요하지만 reward를 주는 행동은 아님.\n",
    "    * 이 경우 agent는 reward에 도달하기 까지 어떤 action들을 취할지에 대한 학습을 수행해야 한다.\n",
    "    \n",
    "## Cart-Pole Task\n",
    "* 넘어지지않고 pole의 균형을 최대한 맞춰야 한다.\n",
    "* two-armed bandit과는 다르게 다음과 같은 task들이 필요하다:\n",
    "    * Observations:\n",
    "        * pole이 현재 어디에 위치해 있는지, 균형이 맞춰졌을 때의 각도는 어떻게 되는지를 알아야 한다.\n",
    "        * 이를 위해 network는 observation을 해 action의 probability를 생성해낼 때 이를 사용한다.\n",
    "    * Delayed reward\n",
    "        * pole이 균형을 잡고 있다는 것은 현재 뿐만 아니라 미래에도 advantageous하다는 의미\n",
    "        * 이를 위해 시간에 따른 action의 가중치를 부여하는 함수를 이용해 각 observation-action pair에 대한 reward value를 조정해 나갈 것이다.\n",
    "* 시간에 따른 reward를 고려하기 위해 이전에 사용했던 Policy Gradient을 약간 수정한다.\n",
    "    * 각 시각에 하나 이상의 experience를 이용해 업데이트를 수행한다.\n",
    "        * 버퍼에 experience를 저장해놓고 agent를 업데이트할 때 사용한다.\n",
    "        * 이런 experience의 시퀀스를 rollouts 또는 experience traces라고 부른다.\n",
    "            * 이를 통해 immediate reward 뿐만 아니라 그 뒤에 뒤따를 reward도 함께 고려할 수 있다.\n",
    "            \n",
    "## Simple Reinforcement Learning in Tensorflow Part 2-b:\n",
    "### Vanilla Policy Gradient Agent\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Policy-Based Agent\n",
    "\"\"\"\n",
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"1D float array of reward를 만들고 discounted reward를 계산한다.\"\"\"\n",
    "    discounted_r =np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # network의 feed-forward 부분\n",
    "        # agent는 state를 이용해 action을 선택한다.\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype = tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in, h_size, biases_initializer = None,\n",
    "                                     activation_fn = tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden, a_size, activation_fn = tf.nn.softmax,\n",
    "                                          biases_initializer = None)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # training\n",
    "        # reward를 네트워크에 feed하고 action을 선택한다.\n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype = tf.int32)\n",
    "        \n",
    "        self.indexes =tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders= []\n",
    "        \n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name = str(idx) + '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e4750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e4750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e4750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e4750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e45d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e45d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e45d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1498e45d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "14.0\n",
      "25.18\n",
      "27.98\n",
      "32.66\n",
      "45.06\n",
      "49.85\n",
      "58.66\n",
      "85.73\n",
      "118.7\n",
      "117.13\n",
      "149.15\n",
      "153.47\n",
      "154.68\n",
      "152.65\n",
      "151.99\n",
      "162.48\n",
      "176.78\n",
      "183.56\n",
      "181.64\n",
      "179.36\n",
      "175.42\n",
      "188.11\n",
      "193.28\n",
      "191.68\n",
      "192.93\n",
      "190.9\n",
      "191.16\n",
      "194.34\n",
      "196.96\n",
      "198.37\n",
      "198.94\n",
      "197.49\n",
      "198.63\n",
      "197.54\n",
      "199.65\n",
      "200.0\n",
      "198.83\n",
      "196.87\n",
      "194.04\n",
      "196.26\n",
      "197.05\n",
      "194.21\n",
      "196.46\n",
      "193.13\n",
      "187.85\n",
      "185.52\n",
      "187.71\n",
      "193.46\n",
      "195.02\n",
      "188.53\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training the Agent\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# action: left or right\n",
    "# state: [0, 1, 2, 3]\n",
    "myAgent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8)\n",
    "\n",
    "total_episodes = 5000\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while(i < total_episodes):\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        \n",
    "        for j in range(max_ep):\n",
    "            # 네트워크 output에 기반해 확률적으로 action을 선택한다.\n",
    "            a_dist = sess.run(myAgent.output, feed_dict = {myAgent.state_in: [s]})\n",
    "            a = np.random.choice(a_dist[0], p = a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "        \n",
    "            # 해당 action을 취했을 때 얻는 reward\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            ep_history.append([s, a, r, s1])\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "\n",
    "            if d == True:\n",
    "                # 네트워크 업데이트\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 2] = discount_rewards(ep_history[:, 2])\n",
    "                feed_dict = {myAgent.reward_holder: ep_history[:, 2], \n",
    "                             myAgent.action_holder: ep_history[:, 1],\n",
    "                             myAgent.state_in: np.vstack(ep_history[:, 0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict = feed_dict)\n",
    "\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict = feed_dict)\n",
    "\n",
    "                    # 버퍼 초기화\n",
    "                    for ix, grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "\n",
    "                break\n",
    "            \n",
    "        if(i%100 == 0):\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "            \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
