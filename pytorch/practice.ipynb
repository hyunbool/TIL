{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
    "## PyTorch: Tensors\n",
    "- Tensor: 개념적으로 numpy 배열과 동일\n",
    "    - N차원 배열\n",
    "    - GPU 활용해 수치 연산 가속화 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 285.5345458984375\n",
      "199 1.9994640350341797\n",
      "299 0.03371996805071831\n",
      "399 0.0009843886364251375\n",
      "499 0.00013144993863534182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N: batch size / H: 은닉층 차원\n",
    "# D_in: 입력 차원 / D_out: 출력 차원\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 랜덤으로 입력과 출력 데이터 생성\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 랜덤으로 가중치 초기화\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # forward: prediction 계산\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # loss 계산하고 출력\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # backpropagation: loss에 따른 w1, w2의 변화도 계산\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor와 autograd\n",
    "- 이렇게 작은 신경망에서 역전파를 직접 구현하는 것은 큰 일이 아니지만, 대규모의 복잡한 신경망에서는 힘든 일이다.\n",
    "- 대신 autograd를 사용하면 신경망에서 역전파 단계의 연산을 자동화할 수 있다.\n",
    "- Autograd를 사용할 때, 신경망의 순전파 단계에서는 연산 그래프를 정의하게 된다.\n",
    "    - 이 그래프의 노드, 텐서, 엣지는 입력 Tensor로부터 출력 Tensor를 만들어내는 함수가 된다.\n",
    "    - 이 그래프를 통해 역전파를 하게 되면 변화도도 쉽게 계산할 수 있다.\n",
    "- 각 Tensor는 연산 그래프에서 노드로 표현된다.\n",
    "    - 만약 x가 x.requires_trad=True인 Tensor라면 x.grad는 어떤 스칼라값에 대한 x의 변화도를 갖는 또 다른 Tensor가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 270.93780517578125\n",
      "199 1.073225498199463\n",
      "299 0.007113119587302208\n",
      "399 0.00018978930893354118\n",
      "499 3.812230352195911e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, h, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, h, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(h, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # clamp: 괄호 안의 값의 범주에 해당하도록 값을 변경\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # autograd를 사용해 역전파 단계 계산: requires_grad=True를 갖는 모든 Tensor에 대한 손실 변화도 계산\n",
    "    # 이후 w1.grad와 w2.grad는 w1와 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 된다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient descent 사용해 가중치 수동으로 갱신\n",
    "    # torch.no_grad()로 감싸는 이유: 가중치들이 requires_grad=True 이지만 autograd에서는 이를 추적할 필요 없기 때문!\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 가중치 갱신 후 수동으로 변화도를 0으로 만든다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새 autograd 함수 정의하기\n",
    "- autograd의 기본 연산자는 실제로 Tensor를 조작하는 2개의 함수\n",
    "    - forward: 입력 Tensor로부터 출력 Tensor를 계산\n",
    "    - backward: 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 전달 받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산한다.\n",
    "- torch.autograd.Function의 서브클래스를 정의하고 forward와 backward 함수를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게 정의할 수 있다.\n",
    "    - 그 후 객체를 생성하고 이를 함수처럼 호출해 입력 데이터를 갖는 Tensor를 전달하는 식으로 새로운 autograd 연산자 사용할 수 있다.\n",
    "    \n",
    "- ReLU를 이용해 비선형적으로 동작하는 사용자 정의 autograd 함수를 정의 + 2계층 신경망에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function 구현하고\n",
    "    Tensor 연산을 하는 forward와 backward를 구현\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환\n",
    "        ctx는 context object로 역전파 연산을 위한 정보 저장에 사용\n",
    "        ctx.save_for_backward를 사용해 역전파 단계에서 사용할\n",
    "        어떠한 객체도 저장(cache)해 둘 수 있다.\n",
    "        \"\"\"\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에 대한\n",
    "        손실의 변화도를 계산\n",
    "        \"\"\"\n",
    "        \n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1488.951904296875\n",
      "199 17.90707778930664\n",
      "299 0.27688342332839966\n",
      "399 0.005417732056230307\n",
      "499 0.0003151114797219634\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
    "    # 여기에 'relu'라는 이름을 붙였습니다.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
