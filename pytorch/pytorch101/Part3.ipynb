{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81a5d39",
   "metadata": {},
   "source": [
    "# Part 3. Deep Learning\n",
    "## 4. 딥러닝 알고리즘\n",
    "### 4.1 Dropout\n",
    "* 과적합과 gradient vanishing 문제를 완화시킬 수 있는 알고리즘\n",
    "* 신경망 학습 과정 중 레이어의 노드를 랜덤하게 drop 함으로써 generalization 효과를 가져오게 하는 테크닉\n",
    "    * 즉 weight matrix에 랜덤하게 일부 column에 0을 집어넣어 연산\n",
    "    * 레이어마다, 또 에포크마다 확률 값 다르게 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5102d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모듈 임포트\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 신경망 모듈 설계 시 필요한 함수 모아둔 모듈\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.nn 모듈 중에서도 자주 이용되는 함수 F로 지정\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\"\"\"\n",
    "장비 확인\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "\"\"\"\n",
    "MNIST 데이터 다운로드\n",
    "\"\"\"\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"../data/MNIST\",\n",
    "                             train = False,\n",
    "                             transform = transforms.ToTensor())\n",
    "\n",
    "# 다운로드 한 데이터셋을 미니배치 단위로 분리해 지정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2af5377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLP 모델 설계\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # view를 이용해 2차원 데이터를 1차원으로 변환(Flatten)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # 두번째 FF 레이어에 전달하기 위해 계산\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd00d5f",
   "metadata": {},
   "source": [
    "* training = self.training\n",
    "    * 학습 상태일 때와 검증 상태일 때 따라 다르게 적용되기 위해 존재하는 파라미터\n",
    "    * 학습할 때는 dropout을 사용하지만 검증 과정에서는 모든 노드를 사용해야하기 때문\n",
    "    * model.train()으로 명시할 때 self.training = True / model.eval()로 명시할 때 self.training = False로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82171c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Optimizer, Objective Function 설정\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 device에 할당\n",
    "model = Net().to(DEVICE)\n",
    "\n",
    "# back propagation을 이용해 파라미터 업데이트 시 이용하는 optimizer 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "# loss 계산\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc264f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLP 모델 학습을 진행해 학습 데이터에 대한 모델 성능 확인하는 함수 정의\n",
    "\"\"\"\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    # train_loader에는 미니배치 단위로 학습에 이용되는 (image, label)이 저장되어 있음. \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 각 파라미터에 할당된 gradient 값을 이용해 파라미터 값 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)] \\t Train Loss: {:.6f}\".format(Epoch, batch_idx * len(image),\n",
    "                                                                                  len(train_loader.dataset), \n",
    "                                                                                  100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\"\"\"\n",
    "학습되는 과정 속에서 검증 데이터에 대한 모델 성능 확인하는 함수 정의\n",
    "\"\"\"\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # no_grad() 사용해 파라미터 업데이트 방지\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            \n",
    "            test_loss += criterion(output, label).item()\n",
    "            \n",
    "            # 크기가 10인 벡터값 중 가장 큰 값에 대응하는 클래스로 예측했다고 판단\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            \n",
    "            # 올바르게 예측한 경우 count\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "    # 평균 loss 값 계산\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a9f3f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)] \t Train Loss: 1.473844\n",
      "Train Epoch: 1 [6400/60000(11%)] \t Train Loss: 1.337460\n",
      "Train Epoch: 1 [12800/60000(21%)] \t Train Loss: 1.414527\n",
      "Train Epoch: 1 [19200/60000(32%)] \t Train Loss: 1.313331\n",
      "Train Epoch: 1 [25600/60000(43%)] \t Train Loss: 1.337585\n",
      "Train Epoch: 1 [32000/60000(53%)] \t Train Loss: 1.293303\n",
      "Train Epoch: 1 [38400/60000(64%)] \t Train Loss: 1.217575\n",
      "Train Epoch: 1 [44800/60000(75%)] \t Train Loss: 1.454268\n",
      "Train Epoch: 1 [51200/60000(85%)] \t Train Loss: 1.148197\n",
      "Train Epoch: 1 [57600/60000(96%)] \t Train Loss: 1.145716\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0286, \tTest Accuracy: 70.420000 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)] \t Train Loss: 0.979661\n",
      "Train Epoch: 2 [6400/60000(11%)] \t Train Loss: 1.136472\n",
      "Train Epoch: 2 [12800/60000(21%)] \t Train Loss: 0.924340\n",
      "Train Epoch: 2 [19200/60000(32%)] \t Train Loss: 1.016725\n",
      "Train Epoch: 2 [25600/60000(43%)] \t Train Loss: 1.111830\n",
      "Train Epoch: 2 [32000/60000(53%)] \t Train Loss: 1.212076\n",
      "Train Epoch: 2 [38400/60000(64%)] \t Train Loss: 0.983283\n",
      "Train Epoch: 2 [44800/60000(75%)] \t Train Loss: 1.248863\n",
      "Train Epoch: 2 [51200/60000(85%)] \t Train Loss: 0.955710\n",
      "Train Epoch: 2 [57600/60000(96%)] \t Train Loss: 0.813563\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0240, \tTest Accuracy: 74.610000 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)] \t Train Loss: 0.955192\n",
      "Train Epoch: 3 [6400/60000(11%)] \t Train Loss: 0.766155\n",
      "Train Epoch: 3 [12800/60000(21%)] \t Train Loss: 0.624566\n",
      "Train Epoch: 3 [19200/60000(32%)] \t Train Loss: 0.953137\n",
      "Train Epoch: 3 [25600/60000(43%)] \t Train Loss: 0.847792\n",
      "Train Epoch: 3 [32000/60000(53%)] \t Train Loss: 0.510832\n",
      "Train Epoch: 3 [38400/60000(64%)] \t Train Loss: 1.268052\n",
      "Train Epoch: 3 [44800/60000(75%)] \t Train Loss: 0.722646\n",
      "Train Epoch: 3 [51200/60000(85%)] \t Train Loss: 1.031213\n",
      "Train Epoch: 3 [57600/60000(96%)] \t Train Loss: 0.800846\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0210, \tTest Accuracy: 79.670000 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)] \t Train Loss: 1.272957\n",
      "Train Epoch: 4 [6400/60000(11%)] \t Train Loss: 0.785402\n",
      "Train Epoch: 4 [12800/60000(21%)] \t Train Loss: 1.212569\n",
      "Train Epoch: 4 [19200/60000(32%)] \t Train Loss: 1.029269\n",
      "Train Epoch: 4 [25600/60000(43%)] \t Train Loss: 0.984644\n",
      "Train Epoch: 4 [32000/60000(53%)] \t Train Loss: 0.969890\n",
      "Train Epoch: 4 [38400/60000(64%)] \t Train Loss: 1.035868\n",
      "Train Epoch: 4 [44800/60000(75%)] \t Train Loss: 0.485161\n",
      "Train Epoch: 4 [51200/60000(85%)] \t Train Loss: 0.530137\n",
      "Train Epoch: 4 [57600/60000(96%)] \t Train Loss: 0.776710\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0184, \tTest Accuracy: 82.440000 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)] \t Train Loss: 1.044689\n",
      "Train Epoch: 5 [6400/60000(11%)] \t Train Loss: 0.674441\n",
      "Train Epoch: 5 [12800/60000(21%)] \t Train Loss: 1.109941\n",
      "Train Epoch: 5 [19200/60000(32%)] \t Train Loss: 0.483495\n",
      "Train Epoch: 5 [25600/60000(43%)] \t Train Loss: 1.012367\n",
      "Train Epoch: 5 [32000/60000(53%)] \t Train Loss: 0.580667\n",
      "Train Epoch: 5 [38400/60000(64%)] \t Train Loss: 0.561799\n",
      "Train Epoch: 5 [44800/60000(75%)] \t Train Loss: 0.770353\n",
      "Train Epoch: 5 [51200/60000(85%)] \t Train Loss: 0.786945\n",
      "Train Epoch: 5 [57600/60000(96%)] \t Train Loss: 0.596333\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0166, \tTest Accuracy: 84.020000 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)] \t Train Loss: 0.552481\n",
      "Train Epoch: 6 [6400/60000(11%)] \t Train Loss: 0.762765\n",
      "Train Epoch: 6 [12800/60000(21%)] \t Train Loss: 0.481725\n",
      "Train Epoch: 6 [19200/60000(32%)] \t Train Loss: 0.475866\n",
      "Train Epoch: 6 [25600/60000(43%)] \t Train Loss: 1.120496\n",
      "Train Epoch: 6 [32000/60000(53%)] \t Train Loss: 0.928427\n",
      "Train Epoch: 6 [38400/60000(64%)] \t Train Loss: 0.398107\n",
      "Train Epoch: 6 [44800/60000(75%)] \t Train Loss: 0.907341\n",
      "Train Epoch: 6 [51200/60000(85%)] \t Train Loss: 0.383380\n",
      "Train Epoch: 6 [57600/60000(96%)] \t Train Loss: 0.697488\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0152, \tTest Accuracy: 85.530000 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)] \t Train Loss: 0.651648\n",
      "Train Epoch: 7 [6400/60000(11%)] \t Train Loss: 0.456251\n",
      "Train Epoch: 7 [12800/60000(21%)] \t Train Loss: 0.427186\n",
      "Train Epoch: 7 [19200/60000(32%)] \t Train Loss: 0.600027\n",
      "Train Epoch: 7 [25600/60000(43%)] \t Train Loss: 1.083961\n",
      "Train Epoch: 7 [32000/60000(53%)] \t Train Loss: 0.619058\n",
      "Train Epoch: 7 [38400/60000(64%)] \t Train Loss: 0.504346\n",
      "Train Epoch: 7 [44800/60000(75%)] \t Train Loss: 0.782384\n",
      "Train Epoch: 7 [51200/60000(85%)] \t Train Loss: 0.645782\n",
      "Train Epoch: 7 [57600/60000(96%)] \t Train Loss: 0.367369\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0141, \tTest Accuracy: 86.610000 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)] \t Train Loss: 0.500555\n",
      "Train Epoch: 8 [6400/60000(11%)] \t Train Loss: 0.570782\n",
      "Train Epoch: 8 [12800/60000(21%)] \t Train Loss: 0.363996\n",
      "Train Epoch: 8 [19200/60000(32%)] \t Train Loss: 0.650399\n",
      "Train Epoch: 8 [25600/60000(43%)] \t Train Loss: 0.493311\n",
      "Train Epoch: 8 [32000/60000(53%)] \t Train Loss: 0.504751\n",
      "Train Epoch: 8 [38400/60000(64%)] \t Train Loss: 0.609345\n",
      "Train Epoch: 8 [44800/60000(75%)] \t Train Loss: 0.315771\n",
      "Train Epoch: 8 [51200/60000(85%)] \t Train Loss: 0.334523\n",
      "Train Epoch: 8 [57600/60000(96%)] \t Train Loss: 0.467554\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0134, \tTest Accuracy: 87.370000 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)] \t Train Loss: 0.678467\n",
      "Train Epoch: 9 [6400/60000(11%)] \t Train Loss: 0.278619\n",
      "Train Epoch: 9 [12800/60000(21%)] \t Train Loss: 0.290801\n",
      "Train Epoch: 9 [19200/60000(32%)] \t Train Loss: 0.541961\n",
      "Train Epoch: 9 [25600/60000(43%)] \t Train Loss: 0.363046\n",
      "Train Epoch: 9 [32000/60000(53%)] \t Train Loss: 0.326988\n",
      "Train Epoch: 9 [38400/60000(64%)] \t Train Loss: 0.564979\n",
      "Train Epoch: 9 [44800/60000(75%)] \t Train Loss: 0.725109\n",
      "Train Epoch: 9 [51200/60000(85%)] \t Train Loss: 0.537564\n",
      "Train Epoch: 9 [57600/60000(96%)] \t Train Loss: 0.599504\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0127, \tTest Accuracy: 87.950000 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)] \t Train Loss: 0.531137\n",
      "Train Epoch: 10 [6400/60000(11%)] \t Train Loss: 0.682627\n",
      "Train Epoch: 10 [12800/60000(21%)] \t Train Loss: 0.661402\n",
      "Train Epoch: 10 [19200/60000(32%)] \t Train Loss: 0.319451\n",
      "Train Epoch: 10 [25600/60000(43%)] \t Train Loss: 0.985840\n",
      "Train Epoch: 10 [32000/60000(53%)] \t Train Loss: 0.663113\n",
      "Train Epoch: 10 [38400/60000(64%)] \t Train Loss: 0.557239\n",
      "Train Epoch: 10 [44800/60000(75%)] \t Train Loss: 0.580290\n",
      "Train Epoch: 10 [51200/60000(85%)] \t Train Loss: 0.582386\n",
      "Train Epoch: 10 [57600/60000(96%)] \t Train Loss: 0.405878\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0123, \tTest Accuracy: 88.160000 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습 수행\n",
    "\"\"\"\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200, )\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696c083",
   "metadata": {},
   "source": [
    "### 4.2 Activation 함수\n",
    "#### 1. ReLU 함수\n",
    "* 기존의 시그모이드 함수와 같은 비선형 활성 함수가 지니고 있는 문제점을 어느 정도 해결한 활성 함수\n",
    "* f(x) = max(0, x)\n",
    "* 입력 값이 0 이상인 부분은 기울기가 1, 0 이하인 부분은 0이 됨\n",
    "    * 역전파 시 곱해지는 activation 미분값이 0 또는 1이 되기 때문에 아예 없애거나 완전히 살릴 수 있음\n",
    "    * 이를 통해 hidden layer가 깊어져도 gradient vanishing이 일어나는 것을 완화시킬 수 있으며, 레이어를 깊게 쌓을 수 있음\n",
    "* Leaky ReLU, ELU, parametric ReLU, SELU, SERLU 등 다양한 Activation 함수들이 파생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b98a4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLP 모델 설계\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # view를 이용해 2차원 데이터를 1차원으로 변환(Flatten)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # 두번째 FF 레이어에 전달하기 위해 계산\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e9e7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Optimizer, Objective Function 설정\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 device에 할당\n",
    "model = Net().to(DEVICE)\n",
    "\n",
    "# back propagation을 이용해 파라미터 업데이트 시 이용하는 optimizer 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "# loss 계산\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34226ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)] \t Train Loss: 2.296834\n",
      "Train Epoch: 1 [6400/60000(11%)] \t Train Loss: 2.012792\n",
      "Train Epoch: 1 [12800/60000(21%)] \t Train Loss: 1.237947\n",
      "Train Epoch: 1 [19200/60000(32%)] \t Train Loss: 0.846841\n",
      "Train Epoch: 1 [25600/60000(43%)] \t Train Loss: 0.814991\n",
      "Train Epoch: 1 [32000/60000(53%)] \t Train Loss: 0.767190\n",
      "Train Epoch: 1 [38400/60000(64%)] \t Train Loss: 0.312187\n",
      "Train Epoch: 1 [44800/60000(75%)] \t Train Loss: 0.369823\n",
      "Train Epoch: 1 [51200/60000(85%)] \t Train Loss: 0.681173\n",
      "Train Epoch: 1 [57600/60000(96%)] \t Train Loss: 0.296593\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0101, \tTest Accuracy: 90.890000 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)] \t Train Loss: 0.458644\n",
      "Train Epoch: 2 [6400/60000(11%)] \t Train Loss: 0.360178\n",
      "Train Epoch: 2 [12800/60000(21%)] \t Train Loss: 0.325507\n",
      "Train Epoch: 2 [19200/60000(32%)] \t Train Loss: 0.296544\n",
      "Train Epoch: 2 [25600/60000(43%)] \t Train Loss: 0.396078\n",
      "Train Epoch: 2 [32000/60000(53%)] \t Train Loss: 0.240016\n",
      "Train Epoch: 2 [38400/60000(64%)] \t Train Loss: 0.320755\n",
      "Train Epoch: 2 [44800/60000(75%)] \t Train Loss: 0.230494\n",
      "Train Epoch: 2 [51200/60000(85%)] \t Train Loss: 0.270020\n",
      "Train Epoch: 2 [57600/60000(96%)] \t Train Loss: 0.343064\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0071, \tTest Accuracy: 93.240000 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)] \t Train Loss: 0.233639\n",
      "Train Epoch: 3 [6400/60000(11%)] \t Train Loss: 0.089075\n",
      "Train Epoch: 3 [12800/60000(21%)] \t Train Loss: 0.320916\n",
      "Train Epoch: 3 [19200/60000(32%)] \t Train Loss: 0.205386\n",
      "Train Epoch: 3 [25600/60000(43%)] \t Train Loss: 0.038238\n",
      "Train Epoch: 3 [32000/60000(53%)] \t Train Loss: 0.221825\n",
      "Train Epoch: 3 [38400/60000(64%)] \t Train Loss: 0.159169\n",
      "Train Epoch: 3 [44800/60000(75%)] \t Train Loss: 0.248166\n",
      "Train Epoch: 3 [51200/60000(85%)] \t Train Loss: 0.247515\n",
      "Train Epoch: 3 [57600/60000(96%)] \t Train Loss: 0.154293\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0055, \tTest Accuracy: 94.860000 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)] \t Train Loss: 0.156731\n",
      "Train Epoch: 4 [6400/60000(11%)] \t Train Loss: 0.168660\n",
      "Train Epoch: 4 [12800/60000(21%)] \t Train Loss: 0.802563\n",
      "Train Epoch: 4 [19200/60000(32%)] \t Train Loss: 0.135824\n",
      "Train Epoch: 4 [25600/60000(43%)] \t Train Loss: 0.165010\n",
      "Train Epoch: 4 [32000/60000(53%)] \t Train Loss: 0.232889\n",
      "Train Epoch: 4 [38400/60000(64%)] \t Train Loss: 0.167605\n",
      "Train Epoch: 4 [44800/60000(75%)] \t Train Loss: 0.141074\n",
      "Train Epoch: 4 [51200/60000(85%)] \t Train Loss: 0.246007\n",
      "Train Epoch: 4 [57600/60000(96%)] \t Train Loss: 0.359172\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0046, \tTest Accuracy: 95.570000 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)] \t Train Loss: 0.428238\n",
      "Train Epoch: 5 [6400/60000(11%)] \t Train Loss: 0.184241\n",
      "Train Epoch: 5 [12800/60000(21%)] \t Train Loss: 0.268768\n",
      "Train Epoch: 5 [19200/60000(32%)] \t Train Loss: 0.063863\n",
      "Train Epoch: 5 [25600/60000(43%)] \t Train Loss: 0.416941\n",
      "Train Epoch: 5 [32000/60000(53%)] \t Train Loss: 0.150849\n",
      "Train Epoch: 5 [38400/60000(64%)] \t Train Loss: 0.140366\n",
      "Train Epoch: 5 [44800/60000(75%)] \t Train Loss: 0.053062\n",
      "Train Epoch: 5 [51200/60000(85%)] \t Train Loss: 0.160771\n",
      "Train Epoch: 5 [57600/60000(96%)] \t Train Loss: 0.189203\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0039, \tTest Accuracy: 96.170000 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)] \t Train Loss: 0.417676\n",
      "Train Epoch: 6 [6400/60000(11%)] \t Train Loss: 0.224366\n",
      "Train Epoch: 6 [12800/60000(21%)] \t Train Loss: 0.462657\n",
      "Train Epoch: 6 [19200/60000(32%)] \t Train Loss: 0.237877\n",
      "Train Epoch: 6 [25600/60000(43%)] \t Train Loss: 0.033914\n",
      "Train Epoch: 6 [32000/60000(53%)] \t Train Loss: 0.166377\n",
      "Train Epoch: 6 [38400/60000(64%)] \t Train Loss: 0.026880\n",
      "Train Epoch: 6 [44800/60000(75%)] \t Train Loss: 0.116207\n",
      "Train Epoch: 6 [51200/60000(85%)] \t Train Loss: 0.185405\n",
      "Train Epoch: 6 [57600/60000(96%)] \t Train Loss: 0.290590\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0036, \tTest Accuracy: 96.410000 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)] \t Train Loss: 0.110921\n",
      "Train Epoch: 7 [6400/60000(11%)] \t Train Loss: 0.154970\n",
      "Train Epoch: 7 [12800/60000(21%)] \t Train Loss: 0.478218\n",
      "Train Epoch: 7 [19200/60000(32%)] \t Train Loss: 0.029847\n",
      "Train Epoch: 7 [25600/60000(43%)] \t Train Loss: 0.163032\n",
      "Train Epoch: 7 [32000/60000(53%)] \t Train Loss: 0.352527\n",
      "Train Epoch: 7 [38400/60000(64%)] \t Train Loss: 0.309235\n",
      "Train Epoch: 7 [44800/60000(75%)] \t Train Loss: 0.053631\n",
      "Train Epoch: 7 [51200/60000(85%)] \t Train Loss: 0.118949\n",
      "Train Epoch: 7 [57600/60000(96%)] \t Train Loss: 0.122259\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0032, \tTest Accuracy: 96.790000 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)] \t Train Loss: 0.027125\n",
      "Train Epoch: 8 [6400/60000(11%)] \t Train Loss: 0.148442\n",
      "Train Epoch: 8 [12800/60000(21%)] \t Train Loss: 0.221986\n",
      "Train Epoch: 8 [19200/60000(32%)] \t Train Loss: 0.170829\n",
      "Train Epoch: 8 [25600/60000(43%)] \t Train Loss: 0.073690\n",
      "Train Epoch: 8 [32000/60000(53%)] \t Train Loss: 0.250244\n",
      "Train Epoch: 8 [38400/60000(64%)] \t Train Loss: 0.013128\n",
      "Train Epoch: 8 [44800/60000(75%)] \t Train Loss: 0.291473\n",
      "Train Epoch: 8 [51200/60000(85%)] \t Train Loss: 0.140900\n",
      "Train Epoch: 8 [57600/60000(96%)] \t Train Loss: 0.064939\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0031, \tTest Accuracy: 97.020000 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)] \t Train Loss: 0.036237\n",
      "Train Epoch: 9 [6400/60000(11%)] \t Train Loss: 0.137235\n",
      "Train Epoch: 9 [12800/60000(21%)] \t Train Loss: 0.063207\n",
      "Train Epoch: 9 [19200/60000(32%)] \t Train Loss: 0.062162\n",
      "Train Epoch: 9 [25600/60000(43%)] \t Train Loss: 0.148765\n",
      "Train Epoch: 9 [32000/60000(53%)] \t Train Loss: 0.206853\n",
      "Train Epoch: 9 [38400/60000(64%)] \t Train Loss: 0.133446\n",
      "Train Epoch: 9 [44800/60000(75%)] \t Train Loss: 0.055397\n",
      "Train Epoch: 9 [51200/60000(85%)] \t Train Loss: 0.190642\n",
      "Train Epoch: 9 [57600/60000(96%)] \t Train Loss: 0.282606\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0028, \tTest Accuracy: 97.200000 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)] \t Train Loss: 0.243810\n",
      "Train Epoch: 10 [6400/60000(11%)] \t Train Loss: 0.023526\n",
      "Train Epoch: 10 [12800/60000(21%)] \t Train Loss: 0.561291\n",
      "Train Epoch: 10 [19200/60000(32%)] \t Train Loss: 0.170138\n",
      "Train Epoch: 10 [25600/60000(43%)] \t Train Loss: 0.046551\n",
      "Train Epoch: 10 [32000/60000(53%)] \t Train Loss: 0.302160\n",
      "Train Epoch: 10 [38400/60000(64%)] \t Train Loss: 0.007107\n",
      "Train Epoch: 10 [44800/60000(75%)] \t Train Loss: 0.157679\n",
      "Train Epoch: 10 [51200/60000(85%)] \t Train Loss: 0.086530\n",
      "Train Epoch: 10 [57600/60000(96%)] \t Train Loss: 0.031877\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0026, \tTest Accuracy: 97.410000 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습 수행\n",
    "\"\"\"\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200, )\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd7b32",
   "metadata": {},
   "source": [
    "### 4.3 Batch Normalization\n",
    "* Internal Covarriance Shift: 각 레이어마다 Input 분포가 달라짐에 따라 학습 속도가 느려지는 현상\n",
    "* Batch Normalization은 이를 방지하기 위한 기법으로 레이어의 Input 분포를 정규화 해 학습 속도를 빠르게 하는 것\n",
    "* 수식: <img src=\"https://latex.codecogs.com/gif.latex?BN%28h%3B%5Cgamma%3B%5Cbeta%29%20%3D%20%5Cbeta%20&plus;%20%5Cgamma%20%5Cfrac%7Bh%20-%20E%28h%29%7D%7B%5Csqrt%7BVar%28h%29&plus;%5Cepsilon%7D%7D\"/>\n",
    "    * h: input의 분포\n",
    "    * beta & gamma: 각각 분포를 shift시키고 scaling 시키는 값으로 역전파를 통해 학습\n",
    "* BN을 통해 분포를 정규화 해 비선형 활성ㅇ 함수의 의미를 살리게 된다!\n",
    "* 차원에 따라 적용되는 함수명이 다르기 때문에 유의해서 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b525ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLP 모델 설계\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # view를 이용해 2차원 데이터를 1차원으로 변환(Flatten)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        # 두번째 FF 레이어에 전달하기 위해 계산\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91fd9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Optimizer, Objective Function 설정\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 device에 할당\n",
    "model = Net().to(DEVICE)\n",
    "\n",
    "# back propagation을 이용해 파라미터 업데이트 시 이용하는 optimizer 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "# loss 계산\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e6d4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)] \t Train Loss: 2.470002\n",
      "Train Epoch: 1 [6400/60000(11%)] \t Train Loss: 0.412125\n",
      "Train Epoch: 1 [12800/60000(21%)] \t Train Loss: 0.648231\n",
      "Train Epoch: 1 [19200/60000(32%)] \t Train Loss: 0.325135\n",
      "Train Epoch: 1 [25600/60000(43%)] \t Train Loss: 0.417800\n",
      "Train Epoch: 1 [32000/60000(53%)] \t Train Loss: 0.317921\n",
      "Train Epoch: 1 [38400/60000(64%)] \t Train Loss: 0.572682\n",
      "Train Epoch: 1 [44800/60000(75%)] \t Train Loss: 0.460256\n",
      "Train Epoch: 1 [51200/60000(85%)] \t Train Loss: 0.363026\n",
      "Train Epoch: 1 [57600/60000(96%)] \t Train Loss: 0.073160\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0047, \tTest Accuracy: 95.560000 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)] \t Train Loss: 0.339551\n",
      "Train Epoch: 2 [6400/60000(11%)] \t Train Loss: 0.129543\n",
      "Train Epoch: 2 [12800/60000(21%)] \t Train Loss: 0.154798\n",
      "Train Epoch: 2 [19200/60000(32%)] \t Train Loss: 0.317309\n",
      "Train Epoch: 2 [25600/60000(43%)] \t Train Loss: 0.320206\n",
      "Train Epoch: 2 [32000/60000(53%)] \t Train Loss: 0.136236\n",
      "Train Epoch: 2 [38400/60000(64%)] \t Train Loss: 0.167839\n",
      "Train Epoch: 2 [44800/60000(75%)] \t Train Loss: 0.349523\n",
      "Train Epoch: 2 [51200/60000(85%)] \t Train Loss: 0.202946\n",
      "Train Epoch: 2 [57600/60000(96%)] \t Train Loss: 0.204203\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0035, \tTest Accuracy: 96.490000 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)] \t Train Loss: 0.107497\n",
      "Train Epoch: 3 [6400/60000(11%)] \t Train Loss: 0.147617\n",
      "Train Epoch: 3 [12800/60000(21%)] \t Train Loss: 0.063145\n",
      "Train Epoch: 3 [19200/60000(32%)] \t Train Loss: 0.350965\n",
      "Train Epoch: 3 [25600/60000(43%)] \t Train Loss: 0.443445\n",
      "Train Epoch: 3 [32000/60000(53%)] \t Train Loss: 0.143753\n",
      "Train Epoch: 3 [38400/60000(64%)] \t Train Loss: 0.368756\n",
      "Train Epoch: 3 [44800/60000(75%)] \t Train Loss: 0.262275\n",
      "Train Epoch: 3 [51200/60000(85%)] \t Train Loss: 0.156408\n",
      "Train Epoch: 3 [57600/60000(96%)] \t Train Loss: 0.121784\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0032, \tTest Accuracy: 96.780000 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)] \t Train Loss: 0.171545\n",
      "Train Epoch: 4 [6400/60000(11%)] \t Train Loss: 0.142581\n",
      "Train Epoch: 4 [12800/60000(21%)] \t Train Loss: 0.124206\n",
      "Train Epoch: 4 [19200/60000(32%)] \t Train Loss: 0.343892\n",
      "Train Epoch: 4 [25600/60000(43%)] \t Train Loss: 0.033880\n",
      "Train Epoch: 4 [32000/60000(53%)] \t Train Loss: 0.042404\n",
      "Train Epoch: 4 [38400/60000(64%)] \t Train Loss: 0.193708\n",
      "Train Epoch: 4 [44800/60000(75%)] \t Train Loss: 0.128270\n",
      "Train Epoch: 4 [51200/60000(85%)] \t Train Loss: 0.087178\n",
      "Train Epoch: 4 [57600/60000(96%)] \t Train Loss: 0.174595\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0027, \tTest Accuracy: 97.430000 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)] \t Train Loss: 0.135053\n",
      "Train Epoch: 5 [6400/60000(11%)] \t Train Loss: 0.057973\n",
      "Train Epoch: 5 [12800/60000(21%)] \t Train Loss: 0.179733\n",
      "Train Epoch: 5 [19200/60000(32%)] \t Train Loss: 0.205592\n",
      "Train Epoch: 5 [25600/60000(43%)] \t Train Loss: 0.274940\n",
      "Train Epoch: 5 [32000/60000(53%)] \t Train Loss: 0.232097\n",
      "Train Epoch: 5 [38400/60000(64%)] \t Train Loss: 0.088661\n",
      "Train Epoch: 5 [44800/60000(75%)] \t Train Loss: 0.037773\n",
      "Train Epoch: 5 [51200/60000(85%)] \t Train Loss: 0.238877\n",
      "Train Epoch: 5 [57600/60000(96%)] \t Train Loss: 0.166192\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0025, \tTest Accuracy: 97.600000 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)] \t Train Loss: 0.120481\n",
      "Train Epoch: 6 [6400/60000(11%)] \t Train Loss: 0.109470\n",
      "Train Epoch: 6 [12800/60000(21%)] \t Train Loss: 0.208505\n",
      "Train Epoch: 6 [19200/60000(32%)] \t Train Loss: 0.356640\n",
      "Train Epoch: 6 [25600/60000(43%)] \t Train Loss: 0.156044\n",
      "Train Epoch: 6 [32000/60000(53%)] \t Train Loss: 0.059790\n",
      "Train Epoch: 6 [38400/60000(64%)] \t Train Loss: 0.085998\n",
      "Train Epoch: 6 [44800/60000(75%)] \t Train Loss: 0.190757\n",
      "Train Epoch: 6 [51200/60000(85%)] \t Train Loss: 0.229444\n",
      "Train Epoch: 6 [57600/60000(96%)] \t Train Loss: 0.222519\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0023, \tTest Accuracy: 97.790000 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)] \t Train Loss: 0.176397\n",
      "Train Epoch: 7 [6400/60000(11%)] \t Train Loss: 0.236691\n",
      "Train Epoch: 7 [12800/60000(21%)] \t Train Loss: 0.030314\n",
      "Train Epoch: 7 [19200/60000(32%)] \t Train Loss: 0.078964\n",
      "Train Epoch: 7 [25600/60000(43%)] \t Train Loss: 0.033592\n",
      "Train Epoch: 7 [32000/60000(53%)] \t Train Loss: 0.133727\n",
      "Train Epoch: 7 [38400/60000(64%)] \t Train Loss: 0.022936\n",
      "Train Epoch: 7 [44800/60000(75%)] \t Train Loss: 0.071530\n",
      "Train Epoch: 7 [51200/60000(85%)] \t Train Loss: 0.247641\n",
      "Train Epoch: 7 [57600/60000(96%)] \t Train Loss: 0.161419\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0022, \tTest Accuracy: 97.780000 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)] \t Train Loss: 0.145480\n",
      "Train Epoch: 8 [6400/60000(11%)] \t Train Loss: 0.052446\n",
      "Train Epoch: 8 [12800/60000(21%)] \t Train Loss: 0.232165\n",
      "Train Epoch: 8 [19200/60000(32%)] \t Train Loss: 0.163940\n",
      "Train Epoch: 8 [25600/60000(43%)] \t Train Loss: 0.040535\n",
      "Train Epoch: 8 [32000/60000(53%)] \t Train Loss: 0.138492\n",
      "Train Epoch: 8 [38400/60000(64%)] \t Train Loss: 0.329367\n",
      "Train Epoch: 8 [44800/60000(75%)] \t Train Loss: 0.073126\n",
      "Train Epoch: 8 [51200/60000(85%)] \t Train Loss: 0.188224\n",
      "Train Epoch: 8 [57600/60000(96%)] \t Train Loss: 0.105193\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0021, \tTest Accuracy: 97.980000 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)] \t Train Loss: 0.185888\n",
      "Train Epoch: 9 [6400/60000(11%)] \t Train Loss: 0.026837\n",
      "Train Epoch: 9 [12800/60000(21%)] \t Train Loss: 0.471174\n",
      "Train Epoch: 9 [19200/60000(32%)] \t Train Loss: 0.133733\n",
      "Train Epoch: 9 [25600/60000(43%)] \t Train Loss: 0.178435\n",
      "Train Epoch: 9 [32000/60000(53%)] \t Train Loss: 0.125437\n",
      "Train Epoch: 9 [38400/60000(64%)] \t Train Loss: 0.032146\n",
      "Train Epoch: 9 [44800/60000(75%)] \t Train Loss: 0.063041\n",
      "Train Epoch: 9 [51200/60000(85%)] \t Train Loss: 0.032666\n",
      "Train Epoch: 9 [57600/60000(96%)] \t Train Loss: 0.138583\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0020, \tTest Accuracy: 97.900000 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)] \t Train Loss: 0.151418\n",
      "Train Epoch: 10 [6400/60000(11%)] \t Train Loss: 0.519947\n",
      "Train Epoch: 10 [12800/60000(21%)] \t Train Loss: 0.059472\n",
      "Train Epoch: 10 [19200/60000(32%)] \t Train Loss: 0.178304\n",
      "Train Epoch: 10 [25600/60000(43%)] \t Train Loss: 0.317415\n",
      "Train Epoch: 10 [32000/60000(53%)] \t Train Loss: 0.014679\n",
      "Train Epoch: 10 [38400/60000(64%)] \t Train Loss: 0.160480\n",
      "Train Epoch: 10 [44800/60000(75%)] \t Train Loss: 0.151132\n",
      "Train Epoch: 10 [51200/60000(85%)] \t Train Loss: 0.040395\n",
      "Train Epoch: 10 [57600/60000(96%)] \t Train Loss: 0.173984\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0020, \tTest Accuracy: 97.910000 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습 수행\n",
    "\"\"\"\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200, )\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0ce76",
   "metadata": {},
   "source": [
    "### 4. Initialization\n",
    "* 신경망을 어떻게 초기화하냐에 따라 학습 속도가 달라진다.\n",
    "* 종류:\n",
    "    * LeCun Initialization\n",
    "        * LeCun Normal Initialization\n",
    "        * LeCun Uniform Initialization\n",
    "    * Xavier Initialization\n",
    "        * 이전 레이어의 노드 수와 다음 레이어의 노드 수에 따라 가중치를 결정\n",
    "    * He Initialization\n",
    "        * Xavier Initialization은 ReLU를 사용할 때 비효율적인데 이를 보완한 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb31963",
   "metadata": {},
   "source": [
    "* pytorch 내의 nn.linear는 어떤 분포에서 샘플링을 통해 파라미터를 초기화 할까?\n",
    "    * output으로 계산되는 벡터의 차원 수의 역수 값에 대한 +/- 범위 내 uniform distribution을 설정해 샘플링\n",
    "* 예제에서는 He Initialization을 이용해 파라미터를 초기화 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15f243c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear): # 레이어 중 nn.Linear인 것에 대해서만 지정\n",
    "        init.kaiming_uniform_(m.weight.data) # he_initialization을 이용해 파라미터 값 초기화\n",
    "        \n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bc1e529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)] \t Train Loss: 3.233096\n",
      "Train Epoch: 1 [6400/60000(11%)] \t Train Loss: 1.050164\n",
      "Train Epoch: 1 [12800/60000(21%)] \t Train Loss: 0.470191\n",
      "Train Epoch: 1 [19200/60000(32%)] \t Train Loss: 0.518469\n",
      "Train Epoch: 1 [25600/60000(43%)] \t Train Loss: 0.489643\n",
      "Train Epoch: 1 [32000/60000(53%)] \t Train Loss: 0.755374\n",
      "Train Epoch: 1 [38400/60000(64%)] \t Train Loss: 0.572384\n",
      "Train Epoch: 1 [44800/60000(75%)] \t Train Loss: 0.570972\n",
      "Train Epoch: 1 [51200/60000(85%)] \t Train Loss: 0.371977\n",
      "Train Epoch: 1 [57600/60000(96%)] \t Train Loss: 0.249508\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0069, \tTest Accuracy: 93.400000 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)] \t Train Loss: 0.700280\n",
      "Train Epoch: 2 [6400/60000(11%)] \t Train Loss: 0.490792\n",
      "Train Epoch: 2 [12800/60000(21%)] \t Train Loss: 0.320310\n",
      "Train Epoch: 2 [19200/60000(32%)] \t Train Loss: 0.201973\n",
      "Train Epoch: 2 [25600/60000(43%)] \t Train Loss: 0.237975\n",
      "Train Epoch: 2 [32000/60000(53%)] \t Train Loss: 0.244611\n",
      "Train Epoch: 2 [38400/60000(64%)] \t Train Loss: 0.599197\n",
      "Train Epoch: 2 [44800/60000(75%)] \t Train Loss: 0.518061\n",
      "Train Epoch: 2 [51200/60000(85%)] \t Train Loss: 0.358613\n",
      "Train Epoch: 2 [57600/60000(96%)] \t Train Loss: 0.528742\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0054, \tTest Accuracy: 94.770000 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)] \t Train Loss: 0.236698\n",
      "Train Epoch: 3 [6400/60000(11%)] \t Train Loss: 0.332883\n",
      "Train Epoch: 3 [12800/60000(21%)] \t Train Loss: 0.936870\n",
      "Train Epoch: 3 [19200/60000(32%)] \t Train Loss: 0.243288\n",
      "Train Epoch: 3 [25600/60000(43%)] \t Train Loss: 0.127690\n",
      "Train Epoch: 3 [32000/60000(53%)] \t Train Loss: 0.461107\n",
      "Train Epoch: 3 [38400/60000(64%)] \t Train Loss: 0.402427\n",
      "Train Epoch: 3 [44800/60000(75%)] \t Train Loss: 0.171514\n",
      "Train Epoch: 3 [51200/60000(85%)] \t Train Loss: 0.398848\n",
      "Train Epoch: 3 [57600/60000(96%)] \t Train Loss: 0.244030\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0047, \tTest Accuracy: 95.480000 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)] \t Train Loss: 0.327382\n",
      "Train Epoch: 4 [6400/60000(11%)] \t Train Loss: 0.620565\n",
      "Train Epoch: 4 [12800/60000(21%)] \t Train Loss: 0.134480\n",
      "Train Epoch: 4 [19200/60000(32%)] \t Train Loss: 0.505878\n",
      "Train Epoch: 4 [25600/60000(43%)] \t Train Loss: 0.162189\n",
      "Train Epoch: 4 [32000/60000(53%)] \t Train Loss: 0.323896\n",
      "Train Epoch: 4 [38400/60000(64%)] \t Train Loss: 0.097637\n",
      "Train Epoch: 4 [44800/60000(75%)] \t Train Loss: 0.285461\n",
      "Train Epoch: 4 [51200/60000(85%)] \t Train Loss: 0.224685\n",
      "Train Epoch: 4 [57600/60000(96%)] \t Train Loss: 0.266674\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0042, \tTest Accuracy: 95.710000 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)] \t Train Loss: 0.054458\n",
      "Train Epoch: 5 [6400/60000(11%)] \t Train Loss: 0.607352\n",
      "Train Epoch: 5 [12800/60000(21%)] \t Train Loss: 0.300969\n",
      "Train Epoch: 5 [19200/60000(32%)] \t Train Loss: 0.360191\n",
      "Train Epoch: 5 [25600/60000(43%)] \t Train Loss: 0.282402\n",
      "Train Epoch: 5 [32000/60000(53%)] \t Train Loss: 0.404508\n",
      "Train Epoch: 5 [38400/60000(64%)] \t Train Loss: 0.083855\n",
      "Train Epoch: 5 [44800/60000(75%)] \t Train Loss: 0.476759\n",
      "Train Epoch: 5 [51200/60000(85%)] \t Train Loss: 0.299541\n",
      "Train Epoch: 5 [57600/60000(96%)] \t Train Loss: 0.390962\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0037, \tTest Accuracy: 96.160000 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)] \t Train Loss: 0.198620\n",
      "Train Epoch: 6 [6400/60000(11%)] \t Train Loss: 0.192744\n",
      "Train Epoch: 6 [12800/60000(21%)] \t Train Loss: 0.242977\n",
      "Train Epoch: 6 [19200/60000(32%)] \t Train Loss: 0.134467\n",
      "Train Epoch: 6 [25600/60000(43%)] \t Train Loss: 0.301805\n",
      "Train Epoch: 6 [32000/60000(53%)] \t Train Loss: 0.130412\n",
      "Train Epoch: 6 [38400/60000(64%)] \t Train Loss: 0.227211\n",
      "Train Epoch: 6 [44800/60000(75%)] \t Train Loss: 0.215405\n",
      "Train Epoch: 6 [51200/60000(85%)] \t Train Loss: 0.218126\n",
      "Train Epoch: 6 [57600/60000(96%)] \t Train Loss: 0.195968\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0034, \tTest Accuracy: 96.710000 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)] \t Train Loss: 0.318318\n",
      "Train Epoch: 7 [6400/60000(11%)] \t Train Loss: 0.128596\n",
      "Train Epoch: 7 [12800/60000(21%)] \t Train Loss: 0.228404\n",
      "Train Epoch: 7 [19200/60000(32%)] \t Train Loss: 0.092207\n",
      "Train Epoch: 7 [25600/60000(43%)] \t Train Loss: 0.297112\n",
      "Train Epoch: 7 [32000/60000(53%)] \t Train Loss: 0.166992\n",
      "Train Epoch: 7 [38400/60000(64%)] \t Train Loss: 0.095984\n",
      "Train Epoch: 7 [44800/60000(75%)] \t Train Loss: 0.104313\n",
      "Train Epoch: 7 [51200/60000(85%)] \t Train Loss: 0.298719\n",
      "Train Epoch: 7 [57600/60000(96%)] \t Train Loss: 0.204348\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0033, \tTest Accuracy: 96.690000 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)] \t Train Loss: 0.268417\n",
      "Train Epoch: 8 [6400/60000(11%)] \t Train Loss: 0.203341\n",
      "Train Epoch: 8 [12800/60000(21%)] \t Train Loss: 0.179656\n",
      "Train Epoch: 8 [19200/60000(32%)] \t Train Loss: 0.146011\n",
      "Train Epoch: 8 [25600/60000(43%)] \t Train Loss: 0.270177\n",
      "Train Epoch: 8 [32000/60000(53%)] \t Train Loss: 0.159005\n",
      "Train Epoch: 8 [38400/60000(64%)] \t Train Loss: 0.086773\n",
      "Train Epoch: 8 [44800/60000(75%)] \t Train Loss: 0.482907\n",
      "Train Epoch: 8 [51200/60000(85%)] \t Train Loss: 0.093916\n",
      "Train Epoch: 8 [57600/60000(96%)] \t Train Loss: 0.043248\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0031, \tTest Accuracy: 96.890000 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)] \t Train Loss: 0.060607\n",
      "Train Epoch: 9 [6400/60000(11%)] \t Train Loss: 0.489411\n",
      "Train Epoch: 9 [12800/60000(21%)] \t Train Loss: 0.380856\n",
      "Train Epoch: 9 [19200/60000(32%)] \t Train Loss: 0.073761\n",
      "Train Epoch: 9 [25600/60000(43%)] \t Train Loss: 0.145254\n",
      "Train Epoch: 9 [32000/60000(53%)] \t Train Loss: 0.135197\n",
      "Train Epoch: 9 [38400/60000(64%)] \t Train Loss: 0.217238\n",
      "Train Epoch: 9 [44800/60000(75%)] \t Train Loss: 0.261224\n",
      "Train Epoch: 9 [51200/60000(85%)] \t Train Loss: 0.120679\n",
      "Train Epoch: 9 [57600/60000(96%)] \t Train Loss: 0.088919\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0029, \tTest Accuracy: 96.990000 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)] \t Train Loss: 0.054707\n",
      "Train Epoch: 10 [6400/60000(11%)] \t Train Loss: 0.158483\n",
      "Train Epoch: 10 [12800/60000(21%)] \t Train Loss: 0.216311\n",
      "Train Epoch: 10 [19200/60000(32%)] \t Train Loss: 0.404024\n",
      "Train Epoch: 10 [25600/60000(43%)] \t Train Loss: 0.066709\n",
      "Train Epoch: 10 [32000/60000(53%)] \t Train Loss: 0.165570\n",
      "Train Epoch: 10 [38400/60000(64%)] \t Train Loss: 0.218517\n",
      "Train Epoch: 10 [44800/60000(75%)] \t Train Loss: 0.116660\n",
      "Train Epoch: 10 [51200/60000(85%)] \t Train Loss: 0.070874\n",
      "Train Epoch: 10 [57600/60000(96%)] \t Train Loss: 0.080080\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0027, \tTest Accuracy: 97.260000 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "학습 수행\n",
    "\"\"\"\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200, )\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc07382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
