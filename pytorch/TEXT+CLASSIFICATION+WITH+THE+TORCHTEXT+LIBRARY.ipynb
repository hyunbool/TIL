{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with the torchtext library(https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "- 텍스트 분류를 위해 torchtext library를 사용하는 방법에 대한 튜토리얼\n",
    "    - torchtext 라이브러리:\n",
    "        - AG_NEWS,\n",
    "        - SogouNews,\n",
    "        - DBpedia,\n",
    "        - YelpReviewPolarity,\n",
    "        - YelpReviewFull,\n",
    "        - YahooAnswers,\n",
    "        - AmazonReviewPolarity,\n",
    "        - AmazonReviewFull\n",
    "- 목표:\n",
    "    - iterator로 raw data에 접근\n",
    "    - raw text string들을 torch.Tensor로 변환하는 데이터 전처리 파이프라인 만들기\n",
    "    - torch.utils.data.DataLoader로 데이터 섞고 iterate\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the raw dataset iterators\n",
    "- torchtext 라이브러리는 raw dataset iterator를 제공한다.\n",
    "    - AG_NEWS 데이터셋: raw data를 (라벨, 텍스트) 튜플로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.csv: 29.5MB [00:39, 746kB/s]                             \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipelines\n",
    "1) raw training 데이터셋으로 vocabulary를 만드는 것\n",
    "    - Vocab 클래스의 argument를 설정함으로써 커스텀 된 vocab를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the text preprocessing pipeline with the tokenizer and vocab\n",
    "# pipeline 함수 만들기\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 3, 31, 5298]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data batch and iterator\n",
    "- torch.utils.data.DataLoader: map-style 데이터셋에 getitem()과 len()을 수행할 때 사용되며, indice/key를 data sample로 표현해준다. shuffle argument가 False일 때, iterable dataset를 다룰 때도 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추가(https://subinium.github.io/pytorch-dataloader/)\n",
    "    - PyTorch는 torch.utils.data.Dataset으로 Custom Dataset을 만들고, DataLoader로 데이터를 불러온다.\n",
    "- DataLoader Parameters\n",
    "    - dataset\n",
    "        - Dataset(torch.utils.data.Dataset의 객체를 사용해야 한다)\n",
    "            - Map-style dataset\n",
    "                - index가 존재하며, data\\[index\\]로 데이터 참조 가능\n",
    "                - \\__getitem\\__과 \\__len\\__선언 필요\n",
    "            - Iterable-style dataset\n",
    "                - random으로 읽기 어렵거나, 데이터에 따라 배치 크기가 달라지는 데이터(dynamic batch size)에 적합\n",
    "                - ex) stream data, real-time log등에 적합\n",
    "                - \\__iter\\__선언 필요\n",
    "    - batch_size\n",
    "        - int, optional, default=1\n",
    "            - batch 크기\n",
    "                - 데이터셋에 50개의 데이터 & batch_size가 10이라면 5번의 iteration을 지나면 모든 데이터 볼 수 있다.\n",
    "                - 반복문을 돌리면 (batch_size, \\*(data.shape))의 형태의 Tensor로 데이터가 반환된다.\n",
    "                - 데이터셋에서 return하는 모든 데이터는 Tensor로 변환되어 온다.\n",
    "    - shuffle\n",
    "        - bool, optional, default=False\n",
    "            - 데이터를 DataLoader에서 섞어서 사용하겠는지를 설정할 수 있음\n",
    "            - Dataset에서 초기화 시 random.shuffle로 섞을 수도 있음\n",
    "    - sampler\n",
    "        - Sampler, optional\n",
    "            - torch.utils.data.Sampler 객체를 사용\n",
    "            - sampler는 index를 컨트롤 하는 방법으로 데이터의 index를 원하는 방식대로 조정한다.\n",
    "                - index를 컨트롤하기 때문에 이때 shuffle 파라미터는 False여야 한다.\n",
    "            - map-style에서 컨트롤하기 위해 사용하며, \\__ㅣlen\\__과 \\__iter\\__를 구현하면 된다. 그 외의 미리 선언된 Sampler는 다음과 같다.\n",
    "                - SequentialSampler : 항상 같은 순서\n",
    "                - RandomSampler : 랜덤, replacemetn 여부 선택 가능, 개수 선택 가능\n",
    "                - SubsetRandomSampler : 랜덤 리스트, 위와 두 조건 불가능\n",
    "                - WeigthRandomSampler : 가중치에 따른 확률\n",
    "                - BatchSampler : batch단위로 sampling 가능\n",
    "                - DistributedSampler : 분산처리(torch.nn.parallel.DistributedDataParallel과 함께 사용)\n",
    "    - num_workers\n",
    "        - int, optional, default=0\n",
    "            - 데이터 로딩에 사용하는 subprocess 개수(멀티 프로세싱)\n",
    "            - default 값은 데이터를 메인 프로세스로 불러오는 것을 의미\n",
    "    - collate_fn\n",
    "         - collable, optional\n",
    "             - map-style 데이터셋에서 sample list를 batch 단위로 바꾸기 위해 필요한 기능\n",
    "             - zero-padding이나 Variable Size 데이터 등 데이터 사이즈를 맞추기 위해 많이 사용한다.\n",
    "    - pin_memory\n",
    "         - bool, optional\n",
    "             - True로 선언하면, DataLoader는 Tensor를 CUDA 고정 메모리에 올린다.\n",
    "    - drop_last\n",
    "         - bool, optional\n",
    "             - batch 단위로 데이터를 불러온다면, batch_size에 따라 마지막 batch의 길이가 달라질 수 있다.\n",
    "                 - data 개수는 27개인데, batch_size = 5 ==> 마지막 batch의 크기는 2가 된다.\n",
    "             - batch의 길이가 다른 경우에 따라 loss를 구하기 귀찮은 경우가 생기며, batch 크기에 따른 의존도 높은 함수를 사용할 때 걱정 되는 경우, 마지막 batch를 사용하지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 데이터를 보내기 전에, 함수 collate_fn로 DataLoader로부터 만들어진 batch sample들을 보내야 한다.\n",
    "    - 함수 collate_fn의 input은 batch size와 batch data이며, 그것들을 data processing pipeline을 통해 전처리한다.\n",
    "    - 따라서 이 함수 colalte_fn은 함수의 윗 레벨에 선언되어야 한다.\n",
    "- 이 튜토리얼의 예제에서는 original data batch input은 list로 pack되며, nn.EmbeddingBag의 input으로 보내기 위해 이 list들을 concatenate 시킨다.\n",
    "    - offset: text tensor에서 각 시퀀스의 beggining index를 표시하기 위해 사용하는 tensor\n",
    "    - label: text entry들의 label을 저장하는 tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        # label_list: 처리한 문장 라벨 넣기\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        # text_list: 처리한 문장 넣기\n",
    "        text_list.append(processed_text)\n",
    "        #print(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "- 이 튜토리얼의 모델은 nn.EmbeddingBag 레이어와 분류를 위한 linear 레이어로 이뤄져 있다.\n",
    "    - nn.EmbeddingBag: default=\"mean\" 시 \"bag\" of embedding의 평균 값을 계산한다.\n",
    "        - text input이 각기 다른 길이를 가지고 있다고 해도 모든 텍스트 길이가 offset에 저장되어 있기 때문에 nn.EmbeddingBag는 패딩을 해줄 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
