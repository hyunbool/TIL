{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with the torchtext library(https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "- 텍스트 분류를 위해 torchtext library를 사용하는 방법에 대한 튜토리얼\n",
    "    - torchtext 라이브러리:\n",
    "        - AG_NEWS,\n",
    "        - SogouNews,\n",
    "        - DBpedia,\n",
    "        - YelpReviewPolarity,\n",
    "        - YelpReviewFull,\n",
    "        - YahooAnswers,\n",
    "        - AmazonReviewPolarity,\n",
    "        - AmazonReviewFull\n",
    "- 목표:\n",
    "    - iterator로 raw data에 접근\n",
    "    - raw text string들을 torch.Tensor로 변환하는 데이터 전처리 파이프라인 만들기\n",
    "    - torch.utils.data.DataLoader로 데이터 섞고 iterate\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the raw dataset iterators\n",
    "- torchtext 라이브러리는 raw dataset iterator를 제공한다.\n",
    "    - AG_NEWS 데이터셋: raw data를 (라벨, 텍스트) 튜플로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipelines\n",
    "1) raw training 데이터셋으로 vocabulary를 만드는 것\n",
    "    - Vocab 클래스의 argument를 설정함으로써 커스텀 된 vocab를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the text preprocessing pipeline with the tokenizer and vocab\n",
    "# pipeline 함수 만들기\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 3, 31, 5298]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data batch and iterator\n",
    "- torch.utils.data.DataLoader: map-style 데이터셋에 getitem()과 len()을 수행할 때 사용되며, indice/key를 data sample로 표현해준다. shuffle argument가 False일 때, iterable dataset를 다룰 때도 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추가(https://subinium.github.io/pytorch-dataloader/)\n",
    "    - PyTorch는 torch.utils.data.Dataset으로 Custom Dataset을 만들고, DataLoader로 데이터를 불러온다.\n",
    "- DataLoader Parameters\n",
    "    - dataset\n",
    "        - Dataset(torch.utils.data.Dataset의 객체를 사용해야 한다)\n",
    "            - Map-style dataset\n",
    "                - index가 존재하며, data\\[index\\]로 데이터 참조 가능\n",
    "                - \\__getitem\\__과 \\__len\\__선언 필요\n",
    "            - Iterable-style dataset\n",
    "                - random으로 읽기 어렵거나, 데이터에 따라 배치 크기가 달라지는 데이터(dynamic batch size)에 적합\n",
    "                - ex) stream data, real-time log등에 적합\n",
    "                - \\__iter\\__선언 필요\n",
    "    - batch_size\n",
    "        - int, optional, default=1\n",
    "            - batch 크기\n",
    "                - 데이터셋에 50개의 데이터 & batch_size가 10이라면 5번의 iteration을 지나면 모든 데이터 볼 수 있다.\n",
    "                - 반복문을 돌리면 (batch_size, \\*(data.shape))의 형태의 Tensor로 데이터가 반환된다.\n",
    "                - 데이터셋에서 return하는 모든 데이터는 Tensor로 변환되어 온다.\n",
    "    - shuffle\n",
    "        - bool, optional, default=False\n",
    "            - 데이터를 DataLoader에서 섞어서 사용하겠는지를 설정할 수 있음\n",
    "            - Dataset에서 초기화 시 random.shuffle로 섞을 수도 있음\n",
    "    - sampler\n",
    "        - Sampler, optional\n",
    "            - torch.utils.data.Sampler 객체를 사용\n",
    "            - sampler는 index를 컨트롤 하는 방법으로 데이터의 index를 원하는 방식대로 조정한다.\n",
    "                - index를 컨트롤하기 때문에 이때 shuffle 파라미터는 False여야 한다.\n",
    "            - map-style에서 컨트롤하기 위해 사용하며, \\__ㅣlen\\__과 \\__iter\\__를 구현하면 된다. 그 외의 미리 선언된 Sampler는 다음과 같다.\n",
    "                - SequentialSampler : 항상 같은 순서\n",
    "                - RandomSampler : 랜덤, replacemetn 여부 선택 가능, 개수 선택 가능\n",
    "                - SubsetRandomSampler : 랜덤 리스트, 위와 두 조건 불가능\n",
    "                - WeigthRandomSampler : 가중치에 따른 확률\n",
    "                - BatchSampler : batch단위로 sampling 가능\n",
    "                - DistributedSampler : 분산처리(torch.nn.parallel.DistributedDataParallel과 함께 사용)\n",
    "    - num_workers\n",
    "        - int, optional, default=0\n",
    "            - 데이터 로딩에 사용하는 subprocess 개수(멀티 프로세싱)\n",
    "            - default 값은 데이터를 메인 프로세스로 불러오는 것을 의미\n",
    "    - collate_fn\n",
    "         - collable, optional\n",
    "             - map-style 데이터셋에서 sample list를 batch 단위로 바꾸기 위해 필요한 기능\n",
    "             - zero-padding이나 Variable Size 데이터 등 데이터 사이즈를 맞추기 위해 많이 사용한다.\n",
    "    - pin_memory\n",
    "         - bool, optional\n",
    "             - True로 선언하면, DataLoader는 Tensor를 CUDA 고정 메모리에 올린다.\n",
    "    - drop_last\n",
    "         - bool, optional\n",
    "             - batch 단위로 데이터를 불러온다면, batch_size에 따라 마지막 batch의 길이가 달라질 수 있다.\n",
    "                 - data 개수는 27개인데, batch_size = 5 ==> 마지막 batch의 크기는 2가 된다.\n",
    "             - batch의 길이가 다른 경우에 따라 loss를 구하기 귀찮은 경우가 생기며, batch 크기에 따른 의존도 높은 함수를 사용할 때 걱정 되는 경우, 마지막 batch를 사용하지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 데이터를 보내기 전에, 함수 collate_fn로 DataLoader로부터 만들어진 batch sample들을 보내야 한다.\n",
    "    - 함수 collate_fn의 input은 batch size와 batch data이며, 그것들을 data processing pipeline을 통해 전처리한다.\n",
    "    - 따라서 이 함수 colalte_fn은 함수의 윗 레벨에 선언되어야 한다.\n",
    "- 이 튜토리얼의 예제에서는 original data batch input은 list로 pack되며, nn.EmbeddingBag의 input으로 보내기 위해 이 list들을 concatenate 시킨다.\n",
    "    - offset: text tensor에서 각 시퀀스의 beggining index를 표시하기 위해 사용하는 tensor\n",
    "    - label: text entry들의 label을 저장하는 tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        # label_list: 처리한 문장 라벨 넣기\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        # text_list: 처리한 문장 넣기\n",
    "        text_list.append(processed_text)\n",
    "        #print(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "- 이 튜토리얼의 모델은 nn.EmbeddingBag 레이어와 분류를 위한 linear 레이어로 이뤄져 있다.\n",
    "    - nn.EmbeddingBag: default=\"mean\" 시 \"bag\" of embedding의 평균 값을 계산한다.\n",
    "        - text input이 각기 다른 길이를 가지고 있다고 해도 모든 텍스트 길이가 offset에 저장되어 있기 때문에 nn.EmbeddingBag는 패딩을 해줄 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded =self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate an instance\n",
    "- AG_NEWS: 총 4개의 클래스로 구성되어 있다.\n",
    "- 임베딩 차원이 64인 모델을 만들어 보자.\n",
    "    - vocab size는 vocabulary instance와 길이가 같다.\n",
    "    - 클래스 개수는 라벨 갯수와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to train model and evaluate results\n",
    "- Torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0)\n",
    "    - parameter의 clip gradient norm\n",
    "    - 모든 gradient에 대해 하나의 vector로 concat해서 norm을 계산한다.\n",
    "    - parameters\n",
    "        - parameters(Iterable[Tensor] or Tensor): gradient를 normalize 할 Tensor\n",
    "        - max_norm(float or int): gradients의 max norm\n",
    "        - norm_type(float or int): p-norm의 타입\n",
    "    - return\n",
    "        - parameter에 대한 전체 norm이 하나의 single vector로 리턴\n",
    "- [Gradient Clipping](https://wikidocs.net/61375#:~:text=%EA%B7%B8%EB%9E%98%EB%94%94%EC%96%B8%ED%8A%B8%20%ED%81%B4%EB%A6%AC%ED%95%91(Gradient%20Clipping),%EC%9D%B4%EB%8A%94%20RNN%EC%97%90%EC%84%9C%20%EC%9C%A0%EC%9A%A9%ED%95%A9%EB%8B%88%EB%8B%A4.)\n",
    "    - 기울기 폭발을막기 위해 임계값을 넘지 않도록 기울기 값을 자르는 것을 말한다.\n",
    "    - 특히 RNN에서 유용한데, RNN은 BPTT 시점에서 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward() # loss를 가지고 backpropagation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # gradient clipping\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('|epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            \n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset and run the model\n",
    "- AG_NEWS는 valid가 없기 때문에 training 데이터셋을 train/valid set으로 나누자.(0.95:0.05)\n",
    "- torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "    - 주어진 길이에 맞도록 랜덤하게 데이터셋을 나눈다.\n",
    "    - parameters\n",
    "        - dataset(Dataset)\n",
    "        - lengths(sequence)\n",
    "        - generator(Generator): random permutation에 사용할 Generator\n",
    "- torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "    - nn.LogSoftmax()와 nn.NLLLoss()를 single 클래스로 결합시켜준다.\n",
    "    - classification 학습 시에 유용하다.\n",
    "    - weight는 각 클래스에 대한 1차원의 할당 weight Tensor이다.\n",
    "- SGD: optimizer로 SGD를 실행한다.\n",
    "- StepLR: epoch 동안 learning rate를 조정하기 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "LR = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|epoch   1 |   500/ 1782 batches | accuracy    0.686\n",
      "|epoch   1 |  1000/ 1782 batches | accuracy    0.853\n",
      "|epoch   1 |  1500/ 1782 batches | accuracy    0.875\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.09s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "|epoch   2 |   500/ 1782 batches | accuracy    0.897\n",
      "|epoch   2 |  1000/ 1782 batches | accuracy    0.899\n",
      "|epoch   2 |  1500/ 1782 batches | accuracy    0.904\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.11s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "|epoch   3 |   500/ 1782 batches | accuracy    0.915\n",
      "|epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
      "|epoch   3 |  1500/ 1782 batches | accuracy    0.911\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.01s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|epoch   4 |   500/ 1782 batches | accuracy    0.928\n",
      "|epoch   4 |  1000/ 1782 batches | accuracy    0.929\n",
      "|epoch   4 |  1500/ 1782 batches | accuracy    0.931\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.18s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n",
      "|epoch   5 |   500/ 1782 batches | accuracy    0.933\n",
      "|epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
      "|epoch   5 |  1500/ 1782 batches | accuracy    0.931\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.08s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "|epoch   6 |   500/ 1782 batches | accuracy    0.932\n",
      "|epoch   6 |  1000/ 1782 batches | accuracy    0.933\n",
      "|epoch   6 |  1500/ 1782 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.99s | valid accuracy    0.917 \n",
      "-----------------------------------------------------------\n",
      "|epoch   7 |   500/ 1782 batches | accuracy    0.933\n",
      "|epoch   7 |  1000/ 1782 batches | accuracy    0.933\n",
      "|epoch   7 |  1500/ 1782 batches | accuracy    0.935\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.12s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "|epoch   8 |   500/ 1782 batches | accuracy    0.935\n",
      "|epoch   8 |  1000/ 1782 batches | accuracy    0.933\n",
      "|epoch   8 |  1500/ 1782 batches | accuracy    0.937\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.02s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "|epoch   9 |   500/ 1782 batches | accuracy    0.937\n",
      "|epoch   9 |  1000/ 1782 batches | accuracy    0.933\n",
      "|epoch   9 |  1500/ 1782 batches | accuracy    0.935\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.45s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n",
      "|epoch  10 |   500/ 1782 batches | accuracy    0.936\n",
      "|epoch  10 |  1000/ 1782 batches | accuracy    0.934\n",
      "|epoch  10 |  1500/ 1782 batches | accuracy    0.935\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.38s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.902\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on a random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
