# 5. 텍스트 유사도

## 모델링

### 1) XG 부스트 텍스트 유사도 분석 모델

#### 소개

* XG 부스트(eXtream Gradient Boosting): 앙상블의 한 방법인 Boosting 기법을 이용하는 방법

* 앙상블 기법: 여러 개의 학습 알고리즘을 사용해 더 좋은 성능을 얻는 방법

  * 배깅과 부스팅이라는 방법이 있음
    * 배깅: 여러 개의 학습 알고리즘, 모델을 통해 각각 결과를 예측하고, 몸든 결과를 동등하게 보고 취합해 결과를 얻는 방식
      * eg) 랜덤 포레스트: 여러 개의 의사결정 트리 결괏값의 평균을 통해 결과를 얻는 배깅 방법 사용
    * 부스팅: 각 결과를 순차적으로 취합하는데, 이때 이전 알고리즘이나 모델이 학습 후 잘못 예측한 부분에 가중치를 두어 다시 모델로 가서 학습하는 방법

  <img src="https://quantdare.com/wp-content/uploads/2016/04/bb3-800x307.png"/>

  

* XG 부스트는 부스팅 기법 중에서도 트리 부스팅(Tree Boosting) 기법을 활용한 모델

  * 트리 부스팅: 여러 개의 의사결정 트리를 사용하지만 단순히 결과를 평균내는 것이 아니라 결과를 보고 오답에 대해 가중치를 부여하며, 가중치가 적용된 오답에 대해서는 관심을 가지고 정답이 될 수 있는 결과를 만들고, 해당 결과에 대한 다른 오답을 찾아 다시 똑같은 작업을 반복적으로 진행한다.

* 이런 트리 부스팅 방식에 Gradient Descent를 통해 최적화 하는 방법

* 계산량을 줄이기 위해 의사결정 트리 구성 시 병렬 처리를 사용해 빠른 시간 안에 학습이 가능

#### 실습

* XG 부스트 모델을 사용하려면 입력값을 xgb 라이브러리의 데이터 형식인 DMatrix 형태로 만들어야 한다.\

  ```python
  import xgboost as xgb
  
train_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)
  eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)

  data_list = [(train_data, 'train'), (eval_data, 'valid')]
  ```
  
  * 적용 과정에서 sum 함수를 사용
    * 각 데이터의 두 질문을 하나의 값을 만들어 주기 위함
  * 그런 다음 두개의 데이터를 묶어 하나의 리스트로 만든다.
    * 이때 학습과 검증 데이터는 각 상태의 문자열과 함께 튜플 형태로 구성한다.



### 2) CNN 텍스트 유사도 분석 모델

### 3) MaLSTM

#### 소개

* 유사도를 구하기 위해 사용하는 대표적인 순환 신경망 계열 모델

* 2016년 조나스 뮐러가 쓴 "Siamese Recurrent Architectures for Learning Sentence Similarity"라는 논문에서 소개됨

* MaLSTM은 Manhattan + LSTM으로 문장 유사도를 구할 때 코사인 유사도 대신 맨하탄 거리를 사용하는 모델

  <img src="https://www.researchgate.net/publication/323622791/figure/fig1/AS:601721235574811@1520472930815/MaLSTM-model-The-MaLSTM-use-a-LSTM-to-read-in-word-vectors-that-represent-each-input.png"/>

  