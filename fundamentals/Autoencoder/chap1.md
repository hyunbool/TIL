# Chap 1. Revisit Deep Neural Networks

* ëª©í‘œ: DNN í•™ìŠµì— ëŒ€í•œ í•´ì„ ì‹œ ML Density Estimationì— ëŒ€í•œ ì´í•´



## ML Problem

![img](https://blog.kakaocdn.net/dn/PLOuX/btqFyQSJrjm/ABsDCJjhIKnFkaJWEIAG8K/img.png)



* ê³¼ì •
  * Collect Traning Data
  * Define functions
  * Learning/Training
  * Predicting/Testing

![img](https://blog.kakaocdn.net/dn/MAEfr/btqFzHtPiuG/1zQHF5bzLzjKq1UKvt3DPK/img.png)

* Define function <= ì´ ë¶€ë¶„ì—ì„œ ëª¨ë¸ë¡œ DNNì„ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ê²ƒ
* í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„°: weight & bias
* Loss ì¸¡ì •: MLE, Cross Entropy
  * ì•„ë¬´ Loss Functionì´ë‚˜ ì“°ì§€ ëª»í•œë‹¤!
  * ì œì•½ì¡°ê±´: Back Propagation
    * ê°€ì •1: Training Data ì „ì²´ loss functionì€ ê° lossì— ëŒ€í•œ í•©
    * ê°€ì •2: Loss functionì˜ inputì€ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ ê°’ê³¼ íƒ€ì¼“ ê°’

![img](https://blog.kakaocdn.net/dn/bTLsq7/btqFz2RZeJy/IWdmXbyKUgkf1dQW0iSef0/img.png)

* Learning/Training: Lossë¥¼ ìµœì†Œí™”í•˜ëŠ” Î¸ë¥¼ ì°¾ëŠ”ë‹¤! => Gradient Descent
  * Iterativeí•˜ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
    * Î¸ì— ëŒ€í•œ ì—…ë°ì´íŠ¸: lossê°€ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™
    * ì–¸ì œ stopí•  ê²ƒì¸ê°€?: lossê°€ ë³€í•˜ì§€ ì•ŠëŠ” ê²½ìš° stop

![img](https://blog.kakaocdn.net/dn/Xz75I/btqFzAPbuQd/yVdpZt09LG43KJcLC4MSsK/img.png)

* ê·¸ëŸ¼ ì–´ë–»ê²Œ Î¸ë¥¼ ì—…ë°ì´íŠ¸?
  * Taylor Expansion: ëª¨ë“  ë¯¸ë¶„ termì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ 1ì°¨ ë¯¸ë¶„ê¹Œì§€ë§Œ ì‚¬ìš©
    * ë” ë§ì€ ì°¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì„ ì‚¬ìš©í•  ìˆ˜ë¡ errorê°€ ì‘ì•„ì§€ê¸´ í•¨!
  * Approximation
    * Loss functionì˜ ì°¨ì´ê°’ì€ <img src="https://latex.codecogs.com/gif.latex?%5Cbigtriangledown%20L%20%5Ccdot%20%5Cbigtriangleup%20%5Ctheta"/>
    * <img src="https://latex.codecogs.com/gif.latex?%5Cbigtriangleup%20%5Ctheta"/> ì— ìŒìˆ˜ê°’ì„ ì·¨í•˜ë©´ loss functionì˜ ê°’ì´ í•­ìƒ ìŒìˆ˜ê°€ ëœë‹¤!



![Deep Neural Networks 5 / 17 01. Collect training data 02. Define functions 03. Learning/Training 04. Predicting/Testing DB...](https://image.slidesharecdn.com/aes171113-180510014736/95/-13-1024.jpg?cb=1525916931)

* 3ë²ˆì§¸ ì¤„: ìƒ˜í”Œë³„ í‰ê·  gradient
* 4ë²ˆì§¸ ì¤„: ê·¸ëŸ°ë° ìƒ˜í”Œ ê°œìˆ˜ê°€ ë§ì•„ì§€ë©´ ì € ê°’ë„ ë„ˆë¬´ ì»¤ì§! ë”°ë¼ì„œ SGD ì‚¬ìš©í•´ Mê°œì— ëŒ€í•œ í‰ê·  gradientë¥¼ êµ¬í•¨



![img](https://blog.kakaocdn.net/dn/PT3cl/btqFz2Et8bT/TeOXBWdJnGInpHcnpq3Cok/img.png)

* Backpropagation
  * ì¶œë ¥ë‹¨ì—ì„œë¶€í„° error signalë¥¼ êµ¬í•œë‹¤.
  * ê° ë ˆì´ì–´ë³„ error signalë¥¼ ìˆœì°¨ì ìœ¼ë¡œ(ì—­ìˆœìœ¼ë¡œ) êµ¬í•œë‹¤.
  * ê° ë ˆì´ì–´ë³„ êµ¬í•œ error signalì— ëŒ€í•œ bias ë¯¸ë¶„ê°’
  * ê° ë ˆì´ì–´ë³„ êµ¬í•œ error signalì— ëŒ€í•œ weight ë¯¸ë¶„ê°’



## Loss Function

### View 1. Back Propagation 

#### 1. MSE

![View-Point I : BackpropagationLOSS FUNCTION 7 / 17 âˆ‘ w b Input : 1.0 Output : 0.0 w=-1.28 b=-0.98 a=+0.09 w =-0.68 b =-0.6...](https://image.slidesharecdn.com/aes171113-180510014736/95/-15-1024.jpg?cb=1525916931)

* weightì™€ biasì— ëŒ€í•œ ì´ˆê¸°ê°’
  * sigmoidì˜ ë¯¸ë¶„ê°’ì´ 0ì— ê°€ê¹Œìš´ì§€ ì—¬ë¶€
    * update í›„ì˜ ê°’ë„ 0ì— ê°€ê¹ê¸° ë•Œë¬¸ì— updateê°€ ì˜ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.
    * ê±°ê¸°ì— ë ˆì´ì–´ ì•ë‹¨ìœ¼ë¡œ ê°ˆ ìˆ˜ë¡ ê°’ì´ ê³„ì† ì‘ì•„ì§
      * Gradient Vanishing ë¬¸ì œ



#### 2. Cross Entropy

![img](https://blog.kakaocdn.net/dn/bneGBr/btqFAqx5QW2/m1PnkKt37y23LDvV4xGR7K/img.png)

* Error Signalì„ êµ¬í•  ë•Œ sigmoid ë¯¸ë¶„ termì´ í¬í•¨ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ˆê¸°ê°’ì— ë¯¼ê°í•˜ì§€ ì•Šê²Œ updateê°€ ì§„í–‰ë  ìˆ˜ ìˆë‹¤.



### View 2. Maximum Likelihood

![img](https://blog.kakaocdn.net/dn/SxjkD/btqFA575iCz/nV3RKGsJYMTSJwUXkzndi0/img.png)

* ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ê°’ì— ëŒ€í•œ í•´ì„ = ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ê°’ì´ ì£¼ì–´ì§ˆ ë•Œ, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ê¸¸(ê°™ê¸¸) ë°”ë¼ê²Œ ëœë‹¤!
  * conditional probability modelì„ ê°€ìš°ì‹œì•ˆìœ¼ë¡œ í•  ë•Œ
    * ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ì€ í™•ë¥ ë¶„í¬ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒ
    * ê°€ìš°ì‹œì•ˆ => ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” í‰ê· ì´ í•„ìš”í•œë°
      * ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ê°’ì„ í‰ê· ìœ¼ë¡œ í•´ì„í–ˆì„ ë•Œ ì´ í‰ê· ê°’ì´ y(ì •ë‹µ)ì´ ê°™ì•„ì§€ëŠ” ê²ƒì„ ì›í•œë‹¤ => Maximum Likelihoodì˜ ê´€ì 

* ì´ë ‡ê²Œ í™•ë¥ ì ìœ¼ë¡œ í•´ì„í•˜ê²Œ ë˜ë©´ ìƒ˜í”Œë§ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤!



![View-Point II : Maximum Likelihood 12 / 17 01. Collect training data 02. Define functions 03. Learning/Training 04. Predic...](https://image.slidesharecdn.com/aes171113-180510014736/95/-20-1024.jpg?cb=1525916931)

* NNLLì´ ì•ì„œ ë§í•œ loss functionì˜ ë‘ê°€ì§€ ì¡°ê±´ì„ ì¶©ì¡±ì‹œí‚¤ëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ì.
  * i.i.d condition
    * ê° conditional probabilityì˜ ê³±ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë…ë¦½
    * ê° ìƒ˜í”Œë³„ í™•ë¥ ë¶„í¬ê°€ ëª¨ë‘ ê°™ë‹¤.

![img](https://blog.kakaocdn.net/dn/bZZZmw/btqFz16CKuM/8heAstPUccUdIIZJT5Xlk0/img.png)

* ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ë˜ë©´ loss function = MSEì™€ ë˜‘ê°™ì•„ì§„ë‹¤!
* ë² ë¥´ëˆ„ì´ ë¶„í¬ => CEì™€ ë™ì¼í•œ ìˆ˜ì‹ì´ ëœë‹¤!

![View-Point II : Maximum Likelihood 15 / 17 Gaussian distribution Categorical distribution ğ‘“ğœƒ ğ‘¥ğ‘– = ğœ‡ğ‘– ğ‘“ğœƒ ğ‘¥ğ‘– = ğ‘ğ‘– Distributi...](https://image.slidesharecdn.com/aes171113-180510014736/95/-23-1024.jpg?cb=1525916931)



![img](https://blog.kakaocdn.net/dn/clThSd/btqFzcaagtT/8Jxbl25tslqo7Gm2iwaD2k/img.png)



![View-Point II : Maximum Likelihood 17 / 17 Connection to Autoencoders Autoencoder LOSS FUNCTION REVISIT DNN Variational Au...](https://image.slidesharecdn.com/aes171113-180510014736/95/-25-1024.jpg?cb=1525916931)

